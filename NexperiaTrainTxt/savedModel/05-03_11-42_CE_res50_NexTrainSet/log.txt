Training: Epoch[001/200] Iteration[300/2001] Loss: 1.7369 Acc:64.18%
Training: Epoch[001/200] Iteration[600/2001] Loss: 1.3212 Acc:67.73%
Training: Epoch[001/200] Iteration[900/2001] Loss: 1.1653 Acc:69.27%
Training: Epoch[001/200] Iteration[1200/2001] Loss: 1.0768 Acc:70.64%
Training: Epoch[001/200] Iteration[1500/2001] Loss: 1.0034 Acc:72.27%
Training: Epoch[001/200] Iteration[1800/2001] Loss: 0.9340 Acc:74.10%
Epoch[001/200] Train Acc: 75.18% Valid Acc:87.76% Train loss:0.8934 Valid loss:0.4424 Train fpr:88.83% Valid fpr:30.35% Train AUC:84.29% Valid AUC:98.17% LR:0.010000000000000002
Training: Epoch[002/200] Iteration[300/2001] Loss: 0.6469 Acc:82.47%
Training: Epoch[002/200] Iteration[600/2001] Loss: 0.5797 Acc:83.92%
Training: Epoch[002/200] Iteration[900/2001] Loss: 0.5328 Acc:85.08%
Training: Epoch[002/200] Iteration[1200/2001] Loss: 0.4951 Acc:85.98%
Training: Epoch[002/200] Iteration[1500/2001] Loss: 0.4668 Acc:86.66%
Training: Epoch[002/200] Iteration[1800/2001] Loss: 0.4447 Acc:87.26%
Epoch[002/200] Train Acc: 87.57% Valid Acc:91.43% Train loss:0.4319 Valid loss:0.3407 Train fpr:31.97% Valid fpr:5.58% Train AUC:97.76% Valid AUC:99.53% LR:0.028000000000000004
Training: Epoch[003/200] Iteration[300/2001] Loss: 0.3424 Acc:90.00%
Training: Epoch[003/200] Iteration[600/2001] Loss: 0.3317 Acc:90.28%
Training: Epoch[003/200] Iteration[900/2001] Loss: 0.3241 Acc:90.47%
Training: Epoch[003/200] Iteration[1200/2001] Loss: 0.3135 Acc:90.76%
Training: Epoch[003/200] Iteration[1500/2001] Loss: 0.3112 Acc:90.77%
Training: Epoch[003/200] Iteration[1800/2001] Loss: 0.3025 Acc:91.05%
Epoch[003/200] Train Acc: 91.19% Valid Acc:90.39% Train loss:0.2974 Valid loss:0.3200 Train fpr:12.12% Valid fpr:5.80% Train AUC:99.20% Valid AUC:99.50% LR:0.046000000000000006
Training: Epoch[004/200] Iteration[300/2001] Loss: 0.2794 Acc:91.99%
Training: Epoch[004/200] Iteration[600/2001] Loss: 0.2751 Acc:91.94%
Training: Epoch[004/200] Iteration[900/2001] Loss: 0.2712 Acc:92.01%
Training: Epoch[004/200] Iteration[1200/2001] Loss: 0.2639 Acc:92.11%
Training: Epoch[004/200] Iteration[1500/2001] Loss: 0.2618 Acc:92.12%
Training: Epoch[004/200] Iteration[1800/2001] Loss: 0.2585 Acc:92.17%
Epoch[004/200] Train Acc: 92.21% Valid Acc:90.14% Train loss:0.2568 Valid loss:0.3421 Train fpr:8.60% Valid fpr:5.47% Train AUC:99.43% Valid AUC:99.59% LR:0.064
Training: Epoch[005/200] Iteration[300/2001] Loss: 0.2527 Acc:92.20%
Training: Epoch[005/200] Iteration[600/2001] Loss: 0.2595 Acc:91.97%
Training: Epoch[005/200] Iteration[900/2001] Loss: 0.2522 Acc:92.35%
Training: Epoch[005/200] Iteration[1200/2001] Loss: 0.2462 Acc:92.52%
Training: Epoch[005/200] Iteration[1500/2001] Loss: 0.2412 Acc:92.64%
Training: Epoch[005/200] Iteration[1800/2001] Loss: 0.2367 Acc:92.77%
Epoch[005/200] Train Acc: 92.79% Valid Acc:92.94% Train loss:0.2353 Valid loss:0.2188 Train fpr:6.40% Valid fpr:1.24% Train AUC:99.52% Valid AUC:99.79% LR:0.08200000000000002
Training: Epoch[006/200] Iteration[300/2001] Loss: 0.2481 Acc:92.25%
Training: Epoch[006/200] Iteration[600/2001] Loss: 0.2399 Acc:92.60%
Training: Epoch[006/200] Iteration[900/2001] Loss: 0.2394 Acc:92.56%
Training: Epoch[006/200] Iteration[1200/2001] Loss: 0.2360 Acc:92.67%
Training: Epoch[006/200] Iteration[1500/2001] Loss: 0.2310 Acc:92.82%
Training: Epoch[006/200] Iteration[1800/2001] Loss: 0.2280 Acc:92.91%
Epoch[006/200] Train Acc: 92.97% Valid Acc:94.55% Train loss:0.2262 Valid loss:0.1722 Train fpr:5.90% Valid fpr:0.85% Train AUC:99.58% Valid AUC:99.82% LR:0.1
Training: Epoch[007/200] Iteration[300/2001] Loss: 0.2100 Acc:93.62%
Training: Epoch[007/200] Iteration[600/2001] Loss: 0.2072 Acc:93.61%
Training: Epoch[007/200] Iteration[900/2001] Loss: 0.2076 Acc:93.55%
Training: Epoch[007/200] Iteration[1200/2001] Loss: 0.2059 Acc:93.65%
Training: Epoch[007/200] Iteration[1500/2001] Loss: 0.2077 Acc:93.55%
Training: Epoch[007/200] Iteration[1800/2001] Loss: 0.2053 Acc:93.60%
Epoch[007/200] Train Acc: 93.59% Valid Acc:90.39% Train loss:0.2049 Valid loss:0.3376 Train fpr:4.08% Valid fpr:4.60% Train AUC:99.66% Valid AUC:99.67% LR:0.1
Training: Epoch[008/200] Iteration[300/2001] Loss: 0.2109 Acc:93.48%
Training: Epoch[008/200] Iteration[600/2001] Loss: 0.2035 Acc:93.60%
Training: Epoch[008/200] Iteration[900/2001] Loss: 0.1993 Acc:93.72%
Training: Epoch[008/200] Iteration[1200/2001] Loss: 0.1971 Acc:93.83%
Training: Epoch[008/200] Iteration[1500/2001] Loss: 0.1957 Acc:93.88%
Training: Epoch[008/200] Iteration[1800/2001] Loss: 0.1950 Acc:93.94%
Epoch[008/200] Train Acc: 93.93% Valid Acc:94.19% Train loss:0.1951 Valid loss:0.1882 Train fpr:3.71% Valid fpr:0.91% Train AUC:99.67% Valid AUC:99.84% LR:0.1
Training: Epoch[009/200] Iteration[300/2001] Loss: 0.1798 Acc:94.20%
Training: Epoch[009/200] Iteration[600/2001] Loss: 0.1814 Acc:94.09%
Training: Epoch[009/200] Iteration[900/2001] Loss: 0.1826 Acc:94.02%
Training: Epoch[009/200] Iteration[1200/2001] Loss: 0.1882 Acc:93.93%
Training: Epoch[009/200] Iteration[1500/2001] Loss: 0.1878 Acc:93.98%
Training: Epoch[009/200] Iteration[1800/2001] Loss: 0.1879 Acc:94.04%
Epoch[009/200] Train Acc: 93.99% Valid Acc:94.67% Train loss:0.1899 Valid loss:0.1682 Train fpr:3.23% Valid fpr:0.79% Train AUC:99.70% Valid AUC:99.84% LR:0.1
Training: Epoch[010/200] Iteration[300/2001] Loss: 0.1815 Acc:94.16%
Training: Epoch[010/200] Iteration[600/2001] Loss: 0.1828 Acc:94.10%
Training: Epoch[010/200] Iteration[900/2001] Loss: 0.1825 Acc:94.15%
Training: Epoch[010/200] Iteration[1200/2001] Loss: 0.1810 Acc:94.24%
Training: Epoch[010/200] Iteration[1500/2001] Loss: 0.1849 Acc:94.13%
Training: Epoch[010/200] Iteration[1800/2001] Loss: 0.1831 Acc:94.20%
Epoch[010/200] Train Acc: 94.11% Valid Acc:93.30% Train loss:0.1848 Valid loss:0.2285 Train fpr:2.77% Valid fpr:3.51% Train AUC:99.73% Valid AUC:99.73% LR:0.1
Training: Epoch[011/200] Iteration[300/2001] Loss: 0.1798 Acc:94.24%
Training: Epoch[011/200] Iteration[600/2001] Loss: 0.1852 Acc:93.93%
Training: Epoch[011/200] Iteration[900/2001] Loss: 0.1827 Acc:94.12%
Training: Epoch[011/200] Iteration[1200/2001] Loss: 0.1815 Acc:94.16%
Training: Epoch[011/200] Iteration[1500/2001] Loss: 0.1832 Acc:94.14%
Training: Epoch[011/200] Iteration[1800/2001] Loss: 0.1826 Acc:94.16%
Epoch[011/200] Train Acc: 94.16% Valid Acc:94.01% Train loss:0.1832 Valid loss:0.1877 Train fpr:2.81% Valid fpr:1.54% Train AUC:99.72% Valid AUC:99.81% LR:0.1
Training: Epoch[012/200] Iteration[300/2001] Loss: 0.1833 Acc:94.15%
Training: Epoch[012/200] Iteration[600/2001] Loss: 0.1800 Acc:94.34%
Training: Epoch[012/200] Iteration[900/2001] Loss: 0.1792 Acc:94.33%
Training: Epoch[012/200] Iteration[1200/2001] Loss: 0.1789 Acc:94.38%
Training: Epoch[012/200] Iteration[1500/2001] Loss: 0.1800 Acc:94.38%
Training: Epoch[012/200] Iteration[1800/2001] Loss: 0.1798 Acc:94.39%
Epoch[012/200] Train Acc: 94.38% Valid Acc:95.32% Train loss:0.1798 Valid loss:0.1629 Train fpr:2.36% Valid fpr:0.53% Train AUC:99.74% Valid AUC:99.86% LR:0.1
Training: Epoch[013/200] Iteration[300/2001] Loss: 0.1777 Acc:94.43%
Training: Epoch[013/200] Iteration[600/2001] Loss: 0.1797 Acc:94.29%
Training: Epoch[013/200] Iteration[900/2001] Loss: 0.1804 Acc:94.31%
Training: Epoch[013/200] Iteration[1200/2001] Loss: 0.1819 Acc:94.30%
Training: Epoch[013/200] Iteration[1500/2001] Loss: 0.1822 Acc:94.27%
Training: Epoch[013/200] Iteration[1800/2001] Loss: 0.1803 Acc:94.33%
Epoch[013/200] Train Acc: 94.37% Valid Acc:94.67% Train loss:0.1789 Valid loss:0.1859 Train fpr:2.43% Valid fpr:1.30% Train AUC:99.73% Valid AUC:99.80% LR:0.1
Training: Epoch[014/200] Iteration[300/2001] Loss: 0.1750 Acc:94.41%
Training: Epoch[014/200] Iteration[600/2001] Loss: 0.1783 Acc:94.35%
Training: Epoch[014/200] Iteration[900/2001] Loss: 0.1791 Acc:94.29%
Training: Epoch[014/200] Iteration[1200/2001] Loss: 0.1820 Acc:94.21%
Training: Epoch[014/200] Iteration[1500/2001] Loss: 0.1820 Acc:94.20%
Training: Epoch[014/200] Iteration[1800/2001] Loss: 0.1821 Acc:94.20%
Epoch[014/200] Train Acc: 94.21% Valid Acc:89.81% Train loss:0.1813 Valid loss:0.2780 Train fpr:2.77% Valid fpr:2.66% Train AUC:99.72% Valid AUC:99.73% LR:0.1
Training: Epoch[015/200] Iteration[300/2001] Loss: 0.1729 Acc:94.45%
Training: Epoch[015/200] Iteration[600/2001] Loss: 0.1790 Acc:94.36%
Training: Epoch[015/200] Iteration[900/2001] Loss: 0.1803 Acc:94.30%
Training: Epoch[015/200] Iteration[1200/2001] Loss: 0.1784 Acc:94.33%
Training: Epoch[015/200] Iteration[1500/2001] Loss: 0.1798 Acc:94.29%
Training: Epoch[015/200] Iteration[1800/2001] Loss: 0.1773 Acc:94.38%
Epoch[015/200] Train Acc: 94.37% Valid Acc:94.79% Train loss:0.1778 Valid loss:0.1673 Train fpr:2.31% Valid fpr:0.45% Train AUC:99.74% Valid AUC:99.87% LR:0.1
Training: Epoch[016/200] Iteration[300/2001] Loss: 0.1841 Acc:94.03%
Training: Epoch[016/200] Iteration[600/2001] Loss: 0.1827 Acc:94.11%
Training: Epoch[016/200] Iteration[900/2001] Loss: 0.1820 Acc:94.12%
Training: Epoch[016/200] Iteration[1200/2001] Loss: 0.1795 Acc:94.28%
Training: Epoch[016/200] Iteration[1500/2001] Loss: 0.1779 Acc:94.36%
Training: Epoch[016/200] Iteration[1800/2001] Loss: 0.1772 Acc:94.38%
Epoch[016/200] Train Acc: 94.37% Valid Acc:94.76% Train loss:0.1774 Valid loss:0.1715 Train fpr:2.44% Valid fpr:1.52% Train AUC:99.75% Valid AUC:99.79% LR:0.1
Training: Epoch[017/200] Iteration[300/2001] Loss: 0.1906 Acc:93.97%
Training: Epoch[017/200] Iteration[600/2001] Loss: 0.1771 Acc:94.27%
Training: Epoch[017/200] Iteration[900/2001] Loss: 0.1759 Acc:94.39%
Training: Epoch[017/200] Iteration[1200/2001] Loss: 0.1743 Acc:94.43%
Training: Epoch[017/200] Iteration[1500/2001] Loss: 0.1736 Acc:94.44%
Training: Epoch[017/200] Iteration[1800/2001] Loss: 0.1737 Acc:94.42%
Epoch[017/200] Train Acc: 94.46% Valid Acc:95.27% Train loss:0.1718 Valid loss:0.1590 Train fpr:1.87% Valid fpr:0.34% Train AUC:99.77% Valid AUC:99.86% LR:0.1
Training: Epoch[018/200] Iteration[300/2001] Loss: 0.1623 Acc:94.65%
Training: Epoch[018/200] Iteration[600/2001] Loss: 0.1672 Acc:94.55%
Training: Epoch[018/200] Iteration[900/2001] Loss: 0.1716 Acc:94.52%
Training: Epoch[018/200] Iteration[1200/2001] Loss: 0.1697 Acc:94.60%
Training: Epoch[018/200] Iteration[1500/2001] Loss: 0.1700 Acc:94.59%
Training: Epoch[018/200] Iteration[1800/2001] Loss: 0.1700 Acc:94.58%
Epoch[018/200] Train Acc: 94.60% Valid Acc:94.90% Train loss:0.1698 Valid loss:0.1692 Train fpr:2.11% Valid fpr:0.69% Train AUC:99.75% Valid AUC:99.88% LR:0.1
Training: Epoch[019/200] Iteration[300/2001] Loss: 0.1719 Acc:94.68%
Training: Epoch[019/200] Iteration[600/2001] Loss: 0.1676 Acc:94.70%
Training: Epoch[019/200] Iteration[900/2001] Loss: 0.1661 Acc:94.76%
Training: Epoch[019/200] Iteration[1200/2001] Loss: 0.1679 Acc:94.73%
Training: Epoch[019/200] Iteration[1500/2001] Loss: 0.1701 Acc:94.65%
Training: Epoch[019/200] Iteration[1800/2001] Loss: 0.1712 Acc:94.60%
Epoch[019/200] Train Acc: 94.56% Valid Acc:94.45% Train loss:0.1720 Valid loss:0.1757 Train fpr:2.05% Valid fpr:0.85% Train AUC:99.76% Valid AUC:99.86% LR:0.1
Training: Epoch[020/200] Iteration[300/2001] Loss: 0.1764 Acc:94.58%
Training: Epoch[020/200] Iteration[600/2001] Loss: 0.1731 Acc:94.58%
Training: Epoch[020/200] Iteration[900/2001] Loss: 0.1731 Acc:94.60%
Training: Epoch[020/200] Iteration[1200/2001] Loss: 0.1729 Acc:94.56%
Training: Epoch[020/200] Iteration[1500/2001] Loss: 0.1717 Acc:94.59%
Training: Epoch[020/200] Iteration[1800/2001] Loss: 0.1733 Acc:94.56%
Epoch[020/200] Train Acc: 94.59% Valid Acc:95.31% Train loss:0.1726 Valid loss:0.1466 Train fpr:2.01% Valid fpr:0.18% Train AUC:99.76% Valid AUC:99.88% LR:0.1
Training: Epoch[021/200] Iteration[300/2001] Loss: 0.1693 Acc:94.39%
Training: Epoch[021/200] Iteration[600/2001] Loss: 0.1633 Acc:94.60%
Training: Epoch[021/200] Iteration[900/2001] Loss: 0.1677 Acc:94.56%
Training: Epoch[021/200] Iteration[1200/2001] Loss: 0.1707 Acc:94.39%
Training: Epoch[021/200] Iteration[1500/2001] Loss: 0.1697 Acc:94.45%
Training: Epoch[021/200] Iteration[1800/2001] Loss: 0.1691 Acc:94.49%
Epoch[021/200] Train Acc: 94.52% Valid Acc:94.50% Train loss:0.1691 Valid loss:0.1817 Train fpr:1.87% Valid fpr:1.07% Train AUC:99.78% Valid AUC:99.84% LR:0.1
Training: Epoch[022/200] Iteration[300/2001] Loss: 0.1701 Acc:94.67%
Training: Epoch[022/200] Iteration[600/2001] Loss: 0.1787 Acc:94.38%
Training: Epoch[022/200] Iteration[900/2001] Loss: 0.1740 Acc:94.50%
Training: Epoch[022/200] Iteration[1200/2001] Loss: 0.1707 Acc:94.65%
Training: Epoch[022/200] Iteration[1500/2001] Loss: 0.1678 Acc:94.71%
Training: Epoch[022/200] Iteration[1800/2001] Loss: 0.1689 Acc:94.66%
Epoch[022/200] Train Acc: 94.66% Valid Acc:94.62% Train loss:0.1694 Valid loss:0.1773 Train fpr:1.92% Valid fpr:0.32% Train AUC:99.77% Valid AUC:99.88% LR:0.1
Training: Epoch[023/200] Iteration[300/2001] Loss: 0.1710 Acc:94.61%
Training: Epoch[023/200] Iteration[600/2001] Loss: 0.1699 Acc:94.55%
Training: Epoch[023/200] Iteration[900/2001] Loss: 0.1701 Acc:94.51%
Training: Epoch[023/200] Iteration[1200/2001] Loss: 0.1712 Acc:94.49%
Training: Epoch[023/200] Iteration[1500/2001] Loss: 0.1693 Acc:94.61%
Training: Epoch[023/200] Iteration[1800/2001] Loss: 0.1686 Acc:94.64%
Epoch[023/200] Train Acc: 94.67% Valid Acc:94.22% Train loss:0.1672 Valid loss:0.1819 Train fpr:1.96% Valid fpr:0.69% Train AUC:99.76% Valid AUC:99.86% LR:0.1
Training: Epoch[024/200] Iteration[300/2001] Loss: 0.1597 Acc:94.80%
Training: Epoch[024/200] Iteration[600/2001] Loss: 0.1640 Acc:94.70%
Training: Epoch[024/200] Iteration[900/2001] Loss: 0.1650 Acc:94.71%
Training: Epoch[024/200] Iteration[1200/2001] Loss: 0.1664 Acc:94.70%
Training: Epoch[024/200] Iteration[1500/2001] Loss: 0.1657 Acc:94.72%
Training: Epoch[024/200] Iteration[1800/2001] Loss: 0.1666 Acc:94.69%
Epoch[024/200] Train Acc: 94.68% Valid Acc:93.87% Train loss:0.1666 Valid loss:0.2029 Train fpr:1.71% Valid fpr:1.80% Train AUC:99.78% Valid AUC:99.81% LR:0.1
Training: Epoch[025/200] Iteration[300/2001] Loss: 0.1694 Acc:94.78%
Training: Epoch[025/200] Iteration[600/2001] Loss: 0.1695 Acc:94.84%
Training: Epoch[025/200] Iteration[900/2001] Loss: 0.1679 Acc:94.78%
Training: Epoch[025/200] Iteration[1200/2001] Loss: 0.1678 Acc:94.72%
Training: Epoch[025/200] Iteration[1500/2001] Loss: 0.1666 Acc:94.73%
Training: Epoch[025/200] Iteration[1800/2001] Loss: 0.1650 Acc:94.79%
Epoch[025/200] Train Acc: 94.80% Valid Acc:95.48% Train loss:0.1654 Valid loss:0.1524 Train fpr:1.71% Valid fpr:0.47% Train AUC:99.78% Valid AUC:99.87% LR:0.1
Training: Epoch[026/200] Iteration[300/2001] Loss: 0.1689 Acc:94.68%
Training: Epoch[026/200] Iteration[600/2001] Loss: 0.1663 Acc:94.70%
Training: Epoch[026/200] Iteration[900/2001] Loss: 0.1683 Acc:94.72%
Training: Epoch[026/200] Iteration[1200/2001] Loss: 0.1645 Acc:94.85%
Training: Epoch[026/200] Iteration[1500/2001] Loss: 0.1658 Acc:94.79%
Training: Epoch[026/200] Iteration[1800/2001] Loss: 0.1643 Acc:94.82%
Epoch[026/200] Train Acc: 94.80% Valid Acc:89.41% Train loss:0.1649 Valid loss:0.3172 Train fpr:1.85% Valid fpr:3.10% Train AUC:99.77% Valid AUC:99.62% LR:0.1
Training: Epoch[027/200] Iteration[300/2001] Loss: 0.1617 Acc:94.86%
Training: Epoch[027/200] Iteration[600/2001] Loss: 0.1652 Acc:94.77%
Training: Epoch[027/200] Iteration[900/2001] Loss: 0.1662 Acc:94.70%
Training: Epoch[027/200] Iteration[1200/2001] Loss: 0.1673 Acc:94.69%
Training: Epoch[027/200] Iteration[1500/2001] Loss: 0.1669 Acc:94.72%
Training: Epoch[027/200] Iteration[1800/2001] Loss: 0.1665 Acc:94.74%
Epoch[027/200] Train Acc: 94.72% Valid Acc:94.14% Train loss:0.1670 Valid loss:0.1828 Train fpr:1.79% Valid fpr:1.99% Train AUC:99.76% Valid AUC:99.81% LR:0.1
Training: Epoch[028/200] Iteration[300/2001] Loss: 0.1565 Acc:95.14%
Training: Epoch[028/200] Iteration[600/2001] Loss: 0.1545 Acc:95.11%
Training: Epoch[028/200] Iteration[900/2001] Loss: 0.1578 Acc:95.01%
Training: Epoch[028/200] Iteration[1200/2001] Loss: 0.1612 Acc:94.94%
Training: Epoch[028/200] Iteration[1500/2001] Loss: 0.1612 Acc:94.91%
Training: Epoch[028/200] Iteration[1800/2001] Loss: 0.1619 Acc:94.87%
Epoch[028/200] Train Acc: 94.88% Valid Acc:95.65% Train loss:0.1618 Valid loss:0.1402 Train fpr:1.56% Valid fpr:0.34% Train AUC:99.78% Valid AUC:99.88% LR:0.1
Training: Epoch[029/200] Iteration[300/2001] Loss: 0.1519 Acc:95.04%
Training: Epoch[029/200] Iteration[600/2001] Loss: 0.1597 Acc:94.82%
Training: Epoch[029/200] Iteration[900/2001] Loss: 0.1662 Acc:94.74%
Training: Epoch[029/200] Iteration[1200/2001] Loss: 0.1651 Acc:94.74%
Training: Epoch[029/200] Iteration[1500/2001] Loss: 0.1647 Acc:94.77%
Training: Epoch[029/200] Iteration[1800/2001] Loss: 0.1643 Acc:94.79%
Epoch[029/200] Train Acc: 94.79% Valid Acc:93.84% Train loss:0.1643 Valid loss:0.2538 Train fpr:1.89% Valid fpr:0.75% Train AUC:99.78% Valid AUC:99.88% LR:0.1
Training: Epoch[030/200] Iteration[300/2001] Loss: 0.1535 Acc:95.32%
Training: Epoch[030/200] Iteration[600/2001] Loss: 0.1580 Acc:95.06%
Training: Epoch[030/200] Iteration[900/2001] Loss: 0.1582 Acc:95.00%
Training: Epoch[030/200] Iteration[1200/2001] Loss: 0.1587 Acc:94.96%
Training: Epoch[030/200] Iteration[1500/2001] Loss: 0.1599 Acc:94.93%
Training: Epoch[030/200] Iteration[1800/2001] Loss: 0.1612 Acc:94.90%
Epoch[030/200] Train Acc: 94.85% Valid Acc:94.32% Train loss:0.1627 Valid loss:0.1887 Train fpr:1.60% Valid fpr:1.32% Train AUC:99.78% Valid AUC:99.80% LR:0.1
Training: Epoch[031/200] Iteration[300/2001] Loss: 0.1641 Acc:94.95%
Training: Epoch[031/200] Iteration[600/2001] Loss: 0.1584 Acc:94.93%
Training: Epoch[031/200] Iteration[900/2001] Loss: 0.1617 Acc:94.82%
Training: Epoch[031/200] Iteration[1200/2001] Loss: 0.1625 Acc:94.81%
Training: Epoch[031/200] Iteration[1500/2001] Loss: 0.1634 Acc:94.80%
Training: Epoch[031/200] Iteration[1800/2001] Loss: 0.1622 Acc:94.82%
Epoch[031/200] Train Acc: 94.79% Valid Acc:92.96% Train loss:0.1631 Valid loss:0.2382 Train fpr:1.59% Valid fpr:2.74% Train AUC:99.78% Valid AUC:99.55% LR:0.1
Training: Epoch[032/200] Iteration[300/2001] Loss: 0.1559 Acc:94.78%
Training: Epoch[032/200] Iteration[600/2001] Loss: 0.1584 Acc:94.97%
Training: Epoch[032/200] Iteration[900/2001] Loss: 0.1602 Acc:94.88%
Training: Epoch[032/200] Iteration[1200/2001] Loss: 0.1587 Acc:94.98%
Training: Epoch[032/200] Iteration[1500/2001] Loss: 0.1635 Acc:94.79%
Training: Epoch[032/200] Iteration[1800/2001] Loss: 0.1612 Acc:94.86%
Epoch[032/200] Train Acc: 94.81% Valid Acc:93.92% Train loss:0.1631 Valid loss:0.1894 Train fpr:1.76% Valid fpr:0.93% Train AUC:99.79% Valid AUC:99.82% LR:0.1
Training: Epoch[033/200] Iteration[300/2001] Loss: 0.1612 Acc:95.01%
Training: Epoch[033/200] Iteration[600/2001] Loss: 0.1605 Acc:95.07%
Training: Epoch[033/200] Iteration[900/2001] Loss: 0.1603 Acc:94.97%
Training: Epoch[033/200] Iteration[1200/2001] Loss: 0.1628 Acc:94.85%
Training: Epoch[033/200] Iteration[1500/2001] Loss: 0.1622 Acc:94.87%
Training: Epoch[033/200] Iteration[1800/2001] Loss: 0.1620 Acc:94.91%
Epoch[033/200] Train Acc: 94.89% Valid Acc:87.71% Train loss:0.1632 Valid loss:0.3288 Train fpr:1.46% Valid fpr:0.77% Train AUC:99.78% Valid AUC:99.85% LR:0.1
Training: Epoch[034/200] Iteration[300/2001] Loss: 0.1528 Acc:95.12%
Training: Epoch[034/200] Iteration[600/2001] Loss: 0.1531 Acc:95.21%
Training: Epoch[034/200] Iteration[900/2001] Loss: 0.1576 Acc:95.00%
Training: Epoch[034/200] Iteration[1200/2001] Loss: 0.1594 Acc:94.98%
Training: Epoch[034/200] Iteration[1500/2001] Loss: 0.1615 Acc:94.90%
Training: Epoch[034/200] Iteration[1800/2001] Loss: 0.1600 Acc:94.91%
Epoch[034/200] Train Acc: 94.86% Valid Acc:95.17% Train loss:0.1614 Valid loss:0.1609 Train fpr:1.64% Valid fpr:1.52% Train AUC:99.79% Valid AUC:99.77% LR:0.1
Training: Epoch[035/200] Iteration[300/2001] Loss: 0.1619 Acc:95.10%
Training: Epoch[035/200] Iteration[600/2001] Loss: 0.1731 Acc:94.61%
Training: Epoch[035/200] Iteration[900/2001] Loss: 0.1646 Acc:94.89%
Training: Epoch[035/200] Iteration[1200/2001] Loss: 0.1613 Acc:94.96%
Training: Epoch[035/200] Iteration[1500/2001] Loss: 0.1602 Acc:94.98%
Training: Epoch[035/200] Iteration[1800/2001] Loss: 0.1613 Acc:94.95%
Epoch[035/200] Train Acc: 94.95% Valid Acc:93.79% Train loss:0.1613 Valid loss:0.1791 Train fpr:1.69% Valid fpr:1.56% Train AUC:99.78% Valid AUC:99.82% LR:0.1
Training: Epoch[036/200] Iteration[300/2001] Loss: 0.1599 Acc:94.66%
Training: Epoch[036/200] Iteration[600/2001] Loss: 0.1613 Acc:94.74%
Training: Epoch[036/200] Iteration[900/2001] Loss: 0.1639 Acc:94.72%
Training: Epoch[036/200] Iteration[1200/2001] Loss: 0.1598 Acc:94.84%
Training: Epoch[036/200] Iteration[1500/2001] Loss: 0.1608 Acc:94.85%
Training: Epoch[036/200] Iteration[1800/2001] Loss: 0.1611 Acc:94.83%
Epoch[036/200] Train Acc: 94.87% Valid Acc:95.07% Train loss:0.1603 Valid loss:0.1624 Train fpr:1.33% Valid fpr:0.71% Train AUC:99.80% Valid AUC:99.85% LR:0.1
Training: Epoch[037/200] Iteration[300/2001] Loss: 0.1625 Acc:94.82%
Training: Epoch[037/200] Iteration[600/2001] Loss: 0.1644 Acc:94.70%
Training: Epoch[037/200] Iteration[900/2001] Loss: 0.1660 Acc:94.67%
Training: Epoch[037/200] Iteration[1200/2001] Loss: 0.1665 Acc:94.64%
Training: Epoch[037/200] Iteration[1500/2001] Loss: 0.1659 Acc:94.73%
Training: Epoch[037/200] Iteration[1800/2001] Loss: 0.1654 Acc:94.76%
Epoch[037/200] Train Acc: 94.78% Valid Acc:92.61% Train loss:0.1646 Valid loss:0.2585 Train fpr:1.65% Valid fpr:7.06% Train AUC:99.77% Valid AUC:99.44% LR:0.1
Training: Epoch[038/200] Iteration[300/2001] Loss: 0.1552 Acc:95.05%
Training: Epoch[038/200] Iteration[600/2001] Loss: 0.1509 Acc:95.12%
Training: Epoch[038/200] Iteration[900/2001] Loss: 0.1529 Acc:95.03%
Training: Epoch[038/200] Iteration[1200/2001] Loss: 0.1564 Acc:94.93%
Training: Epoch[038/200] Iteration[1500/2001] Loss: 0.1581 Acc:94.88%
Training: Epoch[038/200] Iteration[1800/2001] Loss: 0.1597 Acc:94.83%
Epoch[038/200] Train Acc: 94.83% Valid Acc:94.91% Train loss:0.1592 Valid loss:0.1668 Train fpr:1.40% Valid fpr:0.67% Train AUC:99.79% Valid AUC:99.83% LR:0.1
Training: Epoch[039/200] Iteration[300/2001] Loss: 0.1525 Acc:95.11%
Training: Epoch[039/200] Iteration[600/2001] Loss: 0.1560 Acc:95.01%
Training: Epoch[039/200] Iteration[900/2001] Loss: 0.1562 Acc:95.01%
Training: Epoch[039/200] Iteration[1200/2001] Loss: 0.1555 Acc:94.99%
Training: Epoch[039/200] Iteration[1500/2001] Loss: 0.1574 Acc:94.95%
Training: Epoch[039/200] Iteration[1800/2001] Loss: 0.1591 Acc:94.91%
Epoch[039/200] Train Acc: 94.91% Valid Acc:94.06% Train loss:0.1592 Valid loss:0.2001 Train fpr:1.37% Valid fpr:0.65% Train AUC:99.80% Valid AUC:99.82% LR:0.1
Training: Epoch[040/200] Iteration[300/2001] Loss: 0.1682 Acc:94.69%
Training: Epoch[040/200] Iteration[600/2001] Loss: 0.1612 Acc:94.88%
Training: Epoch[040/200] Iteration[900/2001] Loss: 0.1622 Acc:94.86%
Training: Epoch[040/200] Iteration[1200/2001] Loss: 0.1600 Acc:94.92%
Training: Epoch[040/200] Iteration[1500/2001] Loss: 0.1601 Acc:94.94%
Training: Epoch[040/200] Iteration[1800/2001] Loss: 0.1609 Acc:94.90%
Epoch[040/200] Train Acc: 94.92% Valid Acc:95.16% Train loss:0.1611 Valid loss:0.1592 Train fpr:1.41% Valid fpr:0.20% Train AUC:99.79% Valid AUC:99.89% LR:0.1
Training: Epoch[041/200] Iteration[300/2001] Loss: 0.1613 Acc:94.77%
Training: Epoch[041/200] Iteration[600/2001] Loss: 0.1566 Acc:95.14%
Training: Epoch[041/200] Iteration[900/2001] Loss: 0.1581 Acc:95.04%
Training: Epoch[041/200] Iteration[1200/2001] Loss: 0.1606 Acc:94.96%
Training: Epoch[041/200] Iteration[1500/2001] Loss: 0.1614 Acc:94.91%
Training: Epoch[041/200] Iteration[1800/2001] Loss: 0.1601 Acc:94.96%
Epoch[041/200] Train Acc: 94.96% Valid Acc:94.16% Train loss:0.1600 Valid loss:0.1804 Train fpr:1.47% Valid fpr:0.41% Train AUC:99.80% Valid AUC:99.90% LR:0.1
Training: Epoch[042/200] Iteration[300/2001] Loss: 0.1632 Acc:94.73%
Training: Epoch[042/200] Iteration[600/2001] Loss: 0.1662 Acc:94.62%
Training: Epoch[042/200] Iteration[900/2001] Loss: 0.1639 Acc:94.72%
Training: Epoch[042/200] Iteration[1200/2001] Loss: 0.1639 Acc:94.71%
Training: Epoch[042/200] Iteration[1500/2001] Loss: 0.1649 Acc:94.69%
Training: Epoch[042/200] Iteration[1800/2001] Loss: 0.1653 Acc:94.73%
Epoch[042/200] Train Acc: 94.72% Valid Acc:92.75% Train loss:0.1647 Valid loss:0.2274 Train fpr:1.74% Valid fpr:3.12% Train AUC:99.77% Valid AUC:99.75% LR:0.1
Training: Epoch[043/200] Iteration[300/2001] Loss: 0.1535 Acc:95.17%
Training: Epoch[043/200] Iteration[600/2001] Loss: 0.1575 Acc:95.00%
Training: Epoch[043/200] Iteration[900/2001] Loss: 0.1574 Acc:95.02%
Training: Epoch[043/200] Iteration[1200/2001] Loss: 0.1588 Acc:94.98%
Training: Epoch[043/200] Iteration[1500/2001] Loss: 0.1604 Acc:94.93%
Training: Epoch[043/200] Iteration[1800/2001] Loss: 0.1593 Acc:94.97%
Epoch[043/200] Train Acc: 94.96% Valid Acc:91.89% Train loss:0.1590 Valid loss:0.2590 Train fpr:1.46% Valid fpr:1.64% Train AUC:99.79% Valid AUC:99.79% LR:0.1
Training: Epoch[044/200] Iteration[300/2001] Loss: 0.1596 Acc:94.97%
Training: Epoch[044/200] Iteration[600/2001] Loss: 0.1582 Acc:94.99%
Training: Epoch[044/200] Iteration[900/2001] Loss: 0.1598 Acc:94.95%
Training: Epoch[044/200] Iteration[1200/2001] Loss: 0.1575 Acc:95.01%
Training: Epoch[044/200] Iteration[1500/2001] Loss: 0.1589 Acc:95.01%
Training: Epoch[044/200] Iteration[1800/2001] Loss: 0.1582 Acc:95.01%
Epoch[044/200] Train Acc: 95.04% Valid Acc:95.18% Train loss:0.1584 Valid loss:0.1576 Train fpr:1.29% Valid fpr:0.63% Train AUC:99.79% Valid AUC:99.85% LR:0.1
Training: Epoch[045/200] Iteration[300/2001] Loss: 0.1545 Acc:95.01%
Training: Epoch[045/200] Iteration[600/2001] Loss: 0.1549 Acc:95.08%
Training: Epoch[045/200] Iteration[900/2001] Loss: 0.1597 Acc:94.99%
Training: Epoch[045/200] Iteration[1200/2001] Loss: 0.1574 Acc:95.10%
Training: Epoch[045/200] Iteration[1500/2001] Loss: 0.1590 Acc:95.01%
Training: Epoch[045/200] Iteration[1800/2001] Loss: 0.1592 Acc:94.97%
Epoch[045/200] Train Acc: 94.90% Valid Acc:94.47% Train loss:0.1602 Valid loss:0.1916 Train fpr:1.42% Valid fpr:0.49% Train AUC:99.80% Valid AUC:99.90% LR:0.1
Training: Epoch[046/200] Iteration[300/2001] Loss: 0.1633 Acc:94.71%
Training: Epoch[046/200] Iteration[600/2001] Loss: 0.1629 Acc:94.71%
Training: Epoch[046/200] Iteration[900/2001] Loss: 0.1631 Acc:94.73%
Training: Epoch[046/200] Iteration[1200/2001] Loss: 0.1626 Acc:94.77%
Training: Epoch[046/200] Iteration[1500/2001] Loss: 0.1654 Acc:94.66%
Training: Epoch[046/200] Iteration[1800/2001] Loss: 0.1646 Acc:94.68%
Epoch[046/200] Train Acc: 94.63% Valid Acc:94.55% Train loss:0.1653 Valid loss:0.1835 Train fpr:1.92% Valid fpr:1.09% Train AUC:99.77% Valid AUC:99.84% LR:0.1
Training: Epoch[047/200] Iteration[300/2001] Loss: 0.1604 Acc:94.73%
Training: Epoch[047/200] Iteration[600/2001] Loss: 0.1555 Acc:95.04%
Training: Epoch[047/200] Iteration[900/2001] Loss: 0.1553 Acc:95.05%
Training: Epoch[047/200] Iteration[1200/2001] Loss: 0.1545 Acc:95.09%
Training: Epoch[047/200] Iteration[1500/2001] Loss: 0.1553 Acc:95.04%
Training: Epoch[047/200] Iteration[1800/2001] Loss: 0.1561 Acc:95.00%
Epoch[047/200] Train Acc: 94.98% Valid Acc:88.09% Train loss:0.1573 Valid loss:0.3447 Train fpr:1.28% Valid fpr:1.42% Train AUC:99.80% Valid AUC:99.80% LR:0.1
Training: Epoch[048/200] Iteration[300/2001] Loss: 0.1584 Acc:94.82%
Training: Epoch[048/200] Iteration[600/2001] Loss: 0.1613 Acc:94.85%
Training: Epoch[048/200] Iteration[900/2001] Loss: 0.1630 Acc:94.88%
Training: Epoch[048/200] Iteration[1200/2001] Loss: 0.1622 Acc:94.87%
Training: Epoch[048/200] Iteration[1500/2001] Loss: 0.1599 Acc:94.91%
Training: Epoch[048/200] Iteration[1800/2001] Loss: 0.1615 Acc:94.84%
Epoch[048/200] Train Acc: 94.90% Valid Acc:96.02% Train loss:0.1605 Valid loss:0.1279 Train fpr:1.51% Valid fpr:0.14% Train AUC:99.78% Valid AUC:99.91% LR:0.1
Training: Epoch[049/200] Iteration[300/2001] Loss: 0.1581 Acc:94.99%
Training: Epoch[049/200] Iteration[600/2001] Loss: 0.1575 Acc:95.02%
Training: Epoch[049/200] Iteration[900/2001] Loss: 0.1573 Acc:94.98%
Training: Epoch[049/200] Iteration[1200/2001] Loss: 0.1580 Acc:94.95%
Training: Epoch[049/200] Iteration[1500/2001] Loss: 0.1567 Acc:94.98%
Training: Epoch[049/200] Iteration[1800/2001] Loss: 0.1576 Acc:94.94%
Epoch[049/200] Train Acc: 94.93% Valid Acc:94.31% Train loss:0.1574 Valid loss:0.1768 Train fpr:1.45% Valid fpr:0.71% Train AUC:99.80% Valid AUC:99.81% LR:0.1
Training: Epoch[050/200] Iteration[300/2001] Loss: 0.1658 Acc:94.78%
Training: Epoch[050/200] Iteration[600/2001] Loss: 0.1572 Acc:95.11%
Training: Epoch[050/200] Iteration[900/2001] Loss: 0.1535 Acc:95.13%
Training: Epoch[050/200] Iteration[1200/2001] Loss: 0.1548 Acc:95.08%
Training: Epoch[050/200] Iteration[1500/2001] Loss: 0.1564 Acc:95.02%
Training: Epoch[050/200] Iteration[1800/2001] Loss: 0.1587 Acc:94.95%
Epoch[050/200] Train Acc: 94.97% Valid Acc:92.98% Train loss:0.1585 Valid loss:0.2181 Train fpr:1.37% Valid fpr:1.64% Train AUC:99.80% Valid AUC:99.82% LR:0.1
Training: Epoch[051/200] Iteration[300/2001] Loss: 0.1718 Acc:94.43%
Training: Epoch[051/200] Iteration[600/2001] Loss: 0.1572 Acc:94.98%
Training: Epoch[051/200] Iteration[900/2001] Loss: 0.1573 Acc:95.05%
Training: Epoch[051/200] Iteration[1200/2001] Loss: 0.1593 Acc:94.99%
Training: Epoch[051/200] Iteration[1500/2001] Loss: 0.1570 Acc:95.07%
Training: Epoch[051/200] Iteration[1800/2001] Loss: 0.1571 Acc:95.06%
Epoch[051/200] Train Acc: 94.99% Valid Acc:94.00% Train loss:0.1582 Valid loss:0.2130 Train fpr:1.35% Valid fpr:1.95% Train AUC:99.79% Valid AUC:99.83% LR:0.1
Training: Epoch[052/200] Iteration[300/2001] Loss: 0.1652 Acc:94.90%
Training: Epoch[052/200] Iteration[600/2001] Loss: 0.1671 Acc:94.76%
Training: Epoch[052/200] Iteration[900/2001] Loss: 0.1630 Acc:94.81%
Training: Epoch[052/200] Iteration[1200/2001] Loss: 0.1639 Acc:94.75%
Training: Epoch[052/200] Iteration[1500/2001] Loss: 0.1615 Acc:94.82%
Training: Epoch[052/200] Iteration[1800/2001] Loss: 0.1616 Acc:94.81%
Epoch[052/200] Train Acc: 94.84% Valid Acc:95.00% Train loss:0.1609 Valid loss:0.1788 Train fpr:1.33% Valid fpr:0.49% Train AUC:99.78% Valid AUC:99.87% LR:0.1
Training: Epoch[053/200] Iteration[300/2001] Loss: 0.1541 Acc:95.19%
Training: Epoch[053/200] Iteration[600/2001] Loss: 0.1503 Acc:95.27%
Training: Epoch[053/200] Iteration[900/2001] Loss: 0.1567 Acc:95.05%
Training: Epoch[053/200] Iteration[1200/2001] Loss: 0.1592 Acc:94.93%
Training: Epoch[053/200] Iteration[1500/2001] Loss: 0.1609 Acc:94.85%
Training: Epoch[053/200] Iteration[1800/2001] Loss: 0.1610 Acc:94.84%
Epoch[053/200] Train Acc: 94.79% Valid Acc:95.11% Train loss:0.1625 Valid loss:0.1460 Train fpr:1.67% Valid fpr:0.10% Train AUC:99.79% Valid AUC:99.91% LR:0.1
Training: Epoch[054/200] Iteration[300/2001] Loss: 0.1531 Acc:95.32%
Training: Epoch[054/200] Iteration[600/2001] Loss: 0.1561 Acc:95.16%
Training: Epoch[054/200] Iteration[900/2001] Loss: 0.1549 Acc:95.09%
Training: Epoch[054/200] Iteration[1200/2001] Loss: 0.1584 Acc:95.02%
Training: Epoch[054/200] Iteration[1500/2001] Loss: 0.1588 Acc:94.95%
Training: Epoch[054/200] Iteration[1800/2001] Loss: 0.1589 Acc:94.95%
Epoch[054/200] Train Acc: 94.98% Valid Acc:95.38% Train loss:0.1582 Valid loss:0.1641 Train fpr:1.13% Valid fpr:0.61% Train AUC:99.81% Valid AUC:99.84% LR:0.1
Training: Epoch[055/200] Iteration[300/2001] Loss: 0.1713 Acc:94.43%
Training: Epoch[055/200] Iteration[600/2001] Loss: 0.1683 Acc:94.48%
Training: Epoch[055/200] Iteration[900/2001] Loss: 0.1618 Acc:94.68%
Training: Epoch[055/200] Iteration[1200/2001] Loss: 0.1648 Acc:94.66%
Training: Epoch[055/200] Iteration[1500/2001] Loss: 0.1631 Acc:94.73%
Training: Epoch[055/200] Iteration[1800/2001] Loss: 0.1597 Acc:94.89%
Epoch[055/200] Train Acc: 94.92% Valid Acc:95.63% Train loss:0.1592 Valid loss:0.1466 Train fpr:1.56% Valid fpr:0.39% Train AUC:99.80% Valid AUC:99.89% LR:0.1
Training: Epoch[056/200] Iteration[300/2001] Loss: 0.1594 Acc:94.81%
Training: Epoch[056/200] Iteration[600/2001] Loss: 0.1597 Acc:94.91%
Training: Epoch[056/200] Iteration[900/2001] Loss: 0.1577 Acc:94.94%
Training: Epoch[056/200] Iteration[1200/2001] Loss: 0.1568 Acc:95.00%
Training: Epoch[056/200] Iteration[1500/2001] Loss: 0.1588 Acc:94.94%
Training: Epoch[056/200] Iteration[1800/2001] Loss: 0.1596 Acc:94.91%
Epoch[056/200] Train Acc: 94.93% Valid Acc:94.44% Train loss:0.1582 Valid loss:0.1739 Train fpr:1.50% Valid fpr:0.67% Train AUC:99.79% Valid AUC:99.87% LR:0.1
Training: Epoch[057/200] Iteration[300/2001] Loss: 0.1571 Acc:94.92%
Training: Epoch[057/200] Iteration[600/2001] Loss: 0.1525 Acc:95.11%
Training: Epoch[057/200] Iteration[900/2001] Loss: 0.1529 Acc:95.08%
Training: Epoch[057/200] Iteration[1200/2001] Loss: 0.1546 Acc:95.07%
Training: Epoch[057/200] Iteration[1500/2001] Loss: 0.1564 Acc:94.97%
Training: Epoch[057/200] Iteration[1800/2001] Loss: 0.1577 Acc:94.92%
Epoch[057/200] Train Acc: 94.89% Valid Acc:91.60% Train loss:0.1588 Valid loss:0.2679 Train fpr:1.49% Valid fpr:1.89% Train AUC:99.79% Valid AUC:99.76% LR:0.1
Training: Epoch[058/200] Iteration[300/2001] Loss: 0.1688 Acc:94.51%
Training: Epoch[058/200] Iteration[600/2001] Loss: 0.1637 Acc:94.71%
Training: Epoch[058/200] Iteration[900/2001] Loss: 0.1632 Acc:94.69%
Training: Epoch[058/200] Iteration[1200/2001] Loss: 0.1612 Acc:94.74%
Training: Epoch[058/200] Iteration[1500/2001] Loss: 0.1590 Acc:94.85%
Training: Epoch[058/200] Iteration[1800/2001] Loss: 0.1591 Acc:94.84%
Epoch[058/200] Train Acc: 94.86% Valid Acc:94.75% Train loss:0.1586 Valid loss:0.1639 Train fpr:1.43% Valid fpr:0.85% Train AUC:99.80% Valid AUC:99.86% LR:0.1
Training: Epoch[059/200] Iteration[300/2001] Loss: 0.1447 Acc:95.59%
Training: Epoch[059/200] Iteration[600/2001] Loss: 0.1528 Acc:95.24%
Training: Epoch[059/200] Iteration[900/2001] Loss: 0.1544 Acc:95.18%
Training: Epoch[059/200] Iteration[1200/2001] Loss: 0.1536 Acc:95.16%
Training: Epoch[059/200] Iteration[1500/2001] Loss: 0.1561 Acc:95.11%
Training: Epoch[059/200] Iteration[1800/2001] Loss: 0.1567 Acc:95.07%
Epoch[059/200] Train Acc: 95.05% Valid Acc:95.47% Train loss:0.1576 Valid loss:0.1457 Train fpr:1.40% Valid fpr:0.20% Train AUC:99.79% Valid AUC:99.89% LR:0.1
Training: Epoch[060/200] Iteration[300/2001] Loss: 0.1610 Acc:94.85%
Training: Epoch[060/200] Iteration[600/2001] Loss: 0.1577 Acc:94.96%
Training: Epoch[060/200] Iteration[900/2001] Loss: 0.1593 Acc:94.92%
Training: Epoch[060/200] Iteration[1200/2001] Loss: 0.1581 Acc:94.94%
Training: Epoch[060/200] Iteration[1500/2001] Loss: 0.1568 Acc:95.03%
Training: Epoch[060/200] Iteration[1800/2001] Loss: 0.1574 Acc:95.02%
Epoch[060/200] Train Acc: 95.03% Valid Acc:95.41% Train loss:0.1568 Valid loss:0.1530 Train fpr:1.40% Valid fpr:0.41% Train AUC:99.79% Valid AUC:99.89% LR:0.1
Training: Epoch[061/200] Iteration[300/2001] Loss: 0.1536 Acc:95.20%
Training: Epoch[061/200] Iteration[600/2001] Loss: 0.1597 Acc:94.93%
Training: Epoch[061/200] Iteration[900/2001] Loss: 0.1588 Acc:94.99%
Training: Epoch[061/200] Iteration[1200/2001] Loss: 0.1601 Acc:94.89%
Training: Epoch[061/200] Iteration[1500/2001] Loss: 0.1587 Acc:94.94%
Training: Epoch[061/200] Iteration[1800/2001] Loss: 0.1602 Acc:94.90%
Epoch[061/200] Train Acc: 94.92% Valid Acc:96.08% Train loss:0.1596 Valid loss:0.1298 Train fpr:1.51% Valid fpr:0.08% Train AUC:99.79% Valid AUC:99.90% LR:0.1
Training: Epoch[062/200] Iteration[300/2001] Loss: 0.1478 Acc:95.21%
Training: Epoch[062/200] Iteration[600/2001] Loss: 0.1558 Acc:94.92%
Training: Epoch[062/200] Iteration[900/2001] Loss: 0.1577 Acc:94.87%
Training: Epoch[062/200] Iteration[1200/2001] Loss: 0.1558 Acc:94.98%
Training: Epoch[062/200] Iteration[1500/2001] Loss: 0.1571 Acc:94.94%
Training: Epoch[062/200] Iteration[1800/2001] Loss: 0.1568 Acc:94.97%
Epoch[062/200] Train Acc: 94.94% Valid Acc:95.77% Train loss:0.1578 Valid loss:0.1365 Train fpr:1.53% Valid fpr:0.06% Train AUC:99.81% Valid AUC:99.91% LR:0.1
Training: Epoch[063/200] Iteration[300/2001] Loss: 0.1457 Acc:95.07%
Training: Epoch[063/200] Iteration[600/2001] Loss: 0.1529 Acc:95.00%
Training: Epoch[063/200] Iteration[900/2001] Loss: 0.1560 Acc:94.86%
Training: Epoch[063/200] Iteration[1200/2001] Loss: 0.1586 Acc:94.81%
Training: Epoch[063/200] Iteration[1500/2001] Loss: 0.1561 Acc:94.93%
Training: Epoch[063/200] Iteration[1800/2001] Loss: 0.1560 Acc:94.96%
Epoch[063/200] Train Acc: 94.95% Valid Acc:93.86% Train loss:0.1561 Valid loss:0.2022 Train fpr:1.34% Valid fpr:0.65% Train AUC:99.81% Valid AUC:99.87% LR:0.1
Training: Epoch[064/200] Iteration[300/2001] Loss: 0.1475 Acc:95.25%
Training: Epoch[064/200] Iteration[600/2001] Loss: 0.1516 Acc:95.10%
Training: Epoch[064/200] Iteration[900/2001] Loss: 0.1537 Acc:95.07%
Training: Epoch[064/200] Iteration[1200/2001] Loss: 0.1545 Acc:95.05%
Training: Epoch[064/200] Iteration[1500/2001] Loss: 0.1566 Acc:94.99%
Training: Epoch[064/200] Iteration[1800/2001] Loss: 0.1565 Acc:94.99%
Epoch[064/200] Train Acc: 94.95% Valid Acc:94.84% Train loss:0.1568 Valid loss:0.1701 Train fpr:1.36% Valid fpr:0.59% Train AUC:99.81% Valid AUC:99.88% LR:0.1
Training: Epoch[065/200] Iteration[300/2001] Loss: 0.1615 Acc:94.93%
Training: Epoch[065/200] Iteration[600/2001] Loss: 0.1602 Acc:95.01%
Training: Epoch[065/200] Iteration[900/2001] Loss: 0.1613 Acc:95.00%
Training: Epoch[065/200] Iteration[1200/2001] Loss: 0.1604 Acc:94.95%
Training: Epoch[065/200] Iteration[1500/2001] Loss: 0.1617 Acc:94.90%
Training: Epoch[065/200] Iteration[1800/2001] Loss: 0.1590 Acc:94.96%
Epoch[065/200] Train Acc: 94.99% Valid Acc:95.06% Train loss:0.1582 Valid loss:0.1643 Train fpr:1.41% Valid fpr:0.61% Train AUC:99.80% Valid AUC:99.88% LR:0.1
Training: Epoch[066/200] Iteration[300/2001] Loss: 0.1734 Acc:94.64%
Training: Epoch[066/200] Iteration[600/2001] Loss: 0.1770 Acc:94.44%
Training: Epoch[066/200] Iteration[900/2001] Loss: 0.1738 Acc:94.57%
Training: Epoch[066/200] Iteration[1200/2001] Loss: 0.1689 Acc:94.66%
Training: Epoch[066/200] Iteration[1500/2001] Loss: 0.1685 Acc:94.70%
Training: Epoch[066/200] Iteration[1800/2001] Loss: 0.1664 Acc:94.73%
Epoch[066/200] Train Acc: 94.77% Valid Acc:95.18% Train loss:0.1643 Valid loss:0.1522 Train fpr:1.59% Valid fpr:0.41% Train AUC:99.77% Valid AUC:99.89% LR:0.1
Training: Epoch[067/200] Iteration[300/2001] Loss: 0.1553 Acc:95.14%
Training: Epoch[067/200] Iteration[600/2001] Loss: 0.1607 Acc:94.98%
Training: Epoch[067/200] Iteration[900/2001] Loss: 0.1590 Acc:95.00%
Training: Epoch[067/200] Iteration[1200/2001] Loss: 0.1612 Acc:94.92%
Training: Epoch[067/200] Iteration[1500/2001] Loss: 0.1574 Acc:95.00%
Training: Epoch[067/200] Iteration[1800/2001] Loss: 0.1572 Acc:94.97%
Epoch[067/200] Train Acc: 95.01% Valid Acc:95.71% Train loss:0.1563 Valid loss:0.1438 Train fpr:1.30% Valid fpr:0.20% Train AUC:99.80% Valid AUC:99.90% LR:0.1
Training: Epoch[068/200] Iteration[300/2001] Loss: 0.1539 Acc:95.05%
Training: Epoch[068/200] Iteration[600/2001] Loss: 0.1569 Acc:94.88%
Training: Epoch[068/200] Iteration[900/2001] Loss: 0.1567 Acc:94.90%
Training: Epoch[068/200] Iteration[1200/2001] Loss: 0.1617 Acc:94.78%
Training: Epoch[068/200] Iteration[1500/2001] Loss: 0.1589 Acc:94.91%
Training: Epoch[068/200] Iteration[1800/2001] Loss: 0.1590 Acc:94.91%
Epoch[068/200] Train Acc: 94.90% Valid Acc:95.43% Train loss:0.1587 Valid loss:0.1461 Train fpr:1.34% Valid fpr:0.51% Train AUC:99.81% Valid AUC:99.84% LR:0.1
Training: Epoch[069/200] Iteration[300/2001] Loss: 0.1608 Acc:94.77%
Training: Epoch[069/200] Iteration[600/2001] Loss: 0.1594 Acc:94.85%
Training: Epoch[069/200] Iteration[900/2001] Loss: 0.1664 Acc:94.64%
Training: Epoch[069/200] Iteration[1200/2001] Loss: 0.1617 Acc:94.79%
Training: Epoch[069/200] Iteration[1500/2001] Loss: 0.1599 Acc:94.84%
Training: Epoch[069/200] Iteration[1800/2001] Loss: 0.1581 Acc:94.92%
Epoch[069/200] Train Acc: 94.92% Valid Acc:94.61% Train loss:0.1581 Valid loss:0.1746 Train fpr:1.24% Valid fpr:0.43% Train AUC:99.80% Valid AUC:99.86% LR:0.1
Training: Epoch[070/200] Iteration[300/2001] Loss: 0.1557 Acc:95.06%
Training: Epoch[070/200] Iteration[600/2001] Loss: 0.1605 Acc:94.83%
Training: Epoch[070/200] Iteration[900/2001] Loss: 0.1617 Acc:94.80%
Training: Epoch[070/200] Iteration[1200/2001] Loss: 0.1600 Acc:94.86%
Training: Epoch[070/200] Iteration[1500/2001] Loss: 0.1585 Acc:94.87%
Training: Epoch[070/200] Iteration[1800/2001] Loss: 0.1576 Acc:94.92%
Epoch[070/200] Train Acc: 94.93% Valid Acc:95.57% Train loss:0.1570 Valid loss:0.1457 Train fpr:1.31% Valid fpr:0.45% Train AUC:99.81% Valid AUC:99.89% LR:0.1
Training: Epoch[071/200] Iteration[300/2001] Loss: 0.1510 Acc:95.12%
Training: Epoch[071/200] Iteration[600/2001] Loss: 0.1556 Acc:94.95%
Training: Epoch[071/200] Iteration[900/2001] Loss: 0.1610 Acc:94.76%
Training: Epoch[071/200] Iteration[1200/2001] Loss: 0.1594 Acc:94.87%
Training: Epoch[071/200] Iteration[1500/2001] Loss: 0.1580 Acc:94.92%
Training: Epoch[071/200] Iteration[1800/2001] Loss: 0.1576 Acc:94.93%
Epoch[071/200] Train Acc: 94.94% Valid Acc:95.63% Train loss:0.1573 Valid loss:0.1495 Train fpr:1.17% Valid fpr:0.36% Train AUC:99.80% Valid AUC:99.87% LR:0.1
Training: Epoch[072/200] Iteration[300/2001] Loss: 0.1381 Acc:95.59%
Training: Epoch[072/200] Iteration[600/2001] Loss: 0.1475 Acc:95.31%
Training: Epoch[072/200] Iteration[900/2001] Loss: 0.1488 Acc:95.29%
Training: Epoch[072/200] Iteration[1200/2001] Loss: 0.1521 Acc:95.27%
Training: Epoch[072/200] Iteration[1500/2001] Loss: 0.1534 Acc:95.23%
Training: Epoch[072/200] Iteration[1800/2001] Loss: 0.1544 Acc:95.16%
Epoch[072/200] Train Acc: 95.14% Valid Acc:93.76% Train loss:0.1540 Valid loss:0.2320 Train fpr:1.25% Valid fpr:1.20% Train AUC:99.81% Valid AUC:99.83% LR:0.1
Training: Epoch[073/200] Iteration[300/2001] Loss: 0.1536 Acc:94.89%
Training: Epoch[073/200] Iteration[600/2001] Loss: 0.1557 Acc:94.94%
Training: Epoch[073/200] Iteration[900/2001] Loss: 0.1639 Acc:94.69%
Training: Epoch[073/200] Iteration[1200/2001] Loss: 0.1629 Acc:94.77%
Training: Epoch[073/200] Iteration[1500/2001] Loss: 0.1597 Acc:94.88%
Training: Epoch[073/200] Iteration[1800/2001] Loss: 0.1606 Acc:94.90%
Epoch[073/200] Train Acc: 94.91% Valid Acc:91.40% Train loss:0.1600 Valid loss:0.2941 Train fpr:1.68% Valid fpr:1.03% Train AUC:99.79% Valid AUC:99.82% LR:0.1
Training: Epoch[074/200] Iteration[300/2001] Loss: 0.1666 Acc:94.54%
Training: Epoch[074/200] Iteration[600/2001] Loss: 0.1671 Acc:94.59%
Training: Epoch[074/200] Iteration[900/2001] Loss: 0.1636 Acc:94.70%
Training: Epoch[074/200] Iteration[1200/2001] Loss: 0.1606 Acc:94.79%
Training: Epoch[074/200] Iteration[1500/2001] Loss: 0.1587 Acc:94.87%
Training: Epoch[074/200] Iteration[1800/2001] Loss: 0.1580 Acc:94.93%
Epoch[074/200] Train Acc: 94.97% Valid Acc:95.90% Train loss:0.1570 Valid loss:0.1392 Train fpr:1.33% Valid fpr:0.24% Train AUC:99.80% Valid AUC:99.91% LR:0.1
Training: Epoch[075/200] Iteration[300/2001] Loss: 0.1630 Acc:94.57%
Training: Epoch[075/200] Iteration[600/2001] Loss: 0.1584 Acc:94.82%
Training: Epoch[075/200] Iteration[900/2001] Loss: 0.1547 Acc:95.06%
Training: Epoch[075/200] Iteration[1200/2001] Loss: 0.1542 Acc:95.09%
Training: Epoch[075/200] Iteration[1500/2001] Loss: 0.1523 Acc:95.15%
Training: Epoch[075/200] Iteration[1800/2001] Loss: 0.1520 Acc:95.13%
Epoch[075/200] Train Acc: 95.08% Valid Acc:94.02% Train loss:0.1540 Valid loss:0.1816 Train fpr:1.27% Valid fpr:0.24% Train AUC:99.81% Valid AUC:99.90% LR:0.1
Training: Epoch[076/200] Iteration[300/2001] Loss: 0.1493 Acc:95.24%
Training: Epoch[076/200] Iteration[600/2001] Loss: 0.1582 Acc:94.90%
Training: Epoch[076/200] Iteration[900/2001] Loss: 0.1597 Acc:94.85%
Training: Epoch[076/200] Iteration[1200/2001] Loss: 0.1610 Acc:94.78%
Training: Epoch[076/200] Iteration[1500/2001] Loss: 0.1615 Acc:94.79%
Training: Epoch[076/200] Iteration[1800/2001] Loss: 0.1605 Acc:94.82%
Epoch[076/200] Train Acc: 94.89% Valid Acc:95.42% Train loss:0.1585 Valid loss:0.1482 Train fpr:1.29% Valid fpr:0.47% Train AUC:99.79% Valid AUC:99.85% LR:0.1
Training: Epoch[077/200] Iteration[300/2001] Loss: 0.1551 Acc:95.04%
Training: Epoch[077/200] Iteration[600/2001] Loss: 0.1600 Acc:94.93%
Training: Epoch[077/200] Iteration[900/2001] Loss: 0.1581 Acc:94.99%
Training: Epoch[077/200] Iteration[1200/2001] Loss: 0.1582 Acc:95.01%
Training: Epoch[077/200] Iteration[1500/2001] Loss: 0.1590 Acc:95.03%
Training: Epoch[077/200] Iteration[1800/2001] Loss: 0.1578 Acc:95.05%
Epoch[077/200] Train Acc: 95.06% Valid Acc:95.70% Train loss:0.1578 Valid loss:0.1321 Train fpr:1.33% Valid fpr:0.10% Train AUC:99.79% Valid AUC:99.91% LR:0.1
Training: Epoch[078/200] Iteration[300/2001] Loss: 0.1475 Acc:95.45%
Training: Epoch[078/200] Iteration[600/2001] Loss: 0.1527 Acc:95.20%
Training: Epoch[078/200] Iteration[900/2001] Loss: 0.1515 Acc:95.25%
Training: Epoch[078/200] Iteration[1200/2001] Loss: 0.1561 Acc:95.11%
Training: Epoch[078/200] Iteration[1500/2001] Loss: 0.1562 Acc:95.07%
Training: Epoch[078/200] Iteration[1800/2001] Loss: 0.1573 Acc:95.06%
Epoch[078/200] Train Acc: 95.03% Valid Acc:95.08% Train loss:0.1583 Valid loss:0.1556 Train fpr:1.42% Valid fpr:0.36% Train AUC:99.80% Valid AUC:99.91% LR:0.1
Training: Epoch[079/200] Iteration[300/2001] Loss: 0.1529 Acc:95.19%
Training: Epoch[079/200] Iteration[600/2001] Loss: 0.1562 Acc:95.04%
Training: Epoch[079/200] Iteration[900/2001] Loss: 0.1544 Acc:95.09%
Training: Epoch[079/200] Iteration[1200/2001] Loss: 0.1548 Acc:95.07%
Training: Epoch[079/200] Iteration[1500/2001] Loss: 0.1574 Acc:94.95%
Training: Epoch[079/200] Iteration[1800/2001] Loss: 0.1553 Acc:95.03%
Epoch[079/200] Train Acc: 95.02% Valid Acc:94.64% Train loss:0.1559 Valid loss:0.1697 Train fpr:1.26% Valid fpr:0.65% Train AUC:99.82% Valid AUC:99.85% LR:0.1
Training: Epoch[080/200] Iteration[300/2001] Loss: 0.1567 Acc:95.04%
Training: Epoch[080/200] Iteration[600/2001] Loss: 0.1603 Acc:95.03%
Training: Epoch[080/200] Iteration[900/2001] Loss: 0.1578 Acc:95.06%
Training: Epoch[080/200] Iteration[1200/2001] Loss: 0.1575 Acc:95.04%
Training: Epoch[080/200] Iteration[1500/2001] Loss: 0.1577 Acc:95.01%
Training: Epoch[080/200] Iteration[1800/2001] Loss: 0.1574 Acc:94.97%
Epoch[080/200] Train Acc: 94.99% Valid Acc:95.75% Train loss:0.1572 Valid loss:0.1374 Train fpr:1.31% Valid fpr:0.22% Train AUC:99.81% Valid AUC:99.90% LR:0.1
Training: Epoch[081/200] Iteration[300/2001] Loss: 0.1500 Acc:95.24%
Training: Epoch[081/200] Iteration[600/2001] Loss: 0.1596 Acc:94.91%
Training: Epoch[081/200] Iteration[900/2001] Loss: 0.1595 Acc:94.88%
Training: Epoch[081/200] Iteration[1200/2001] Loss: 0.1605 Acc:94.83%
Training: Epoch[081/200] Iteration[1500/2001] Loss: 0.1597 Acc:94.82%
Training: Epoch[081/200] Iteration[1800/2001] Loss: 0.1605 Acc:94.77%
Epoch[081/200] Train Acc: 94.83% Valid Acc:95.11% Train loss:0.1597 Valid loss:0.1566 Train fpr:1.59% Valid fpr:0.30% Train AUC:99.80% Valid AUC:99.88% LR:0.1
Training: Epoch[082/200] Iteration[300/2001] Loss: 0.1559 Acc:94.96%
Training: Epoch[082/200] Iteration[600/2001] Loss: 0.1558 Acc:94.95%
Training: Epoch[082/200] Iteration[900/2001] Loss: 0.1548 Acc:95.00%
Training: Epoch[082/200] Iteration[1200/2001] Loss: 0.1553 Acc:94.98%
Training: Epoch[082/200] Iteration[1500/2001] Loss: 0.1583 Acc:94.86%
Training: Epoch[082/200] Iteration[1800/2001] Loss: 0.1582 Acc:94.86%
Epoch[082/200] Train Acc: 94.84% Valid Acc:94.92% Train loss:0.1586 Valid loss:0.1683 Train fpr:1.46% Valid fpr:0.91% Train AUC:99.80% Valid AUC:99.84% LR:0.1
Training: Epoch[083/200] Iteration[300/2001] Loss: 0.1510 Acc:94.99%
Training: Epoch[083/200] Iteration[600/2001] Loss: 0.1595 Acc:94.81%
Training: Epoch[083/200] Iteration[900/2001] Loss: 0.1571 Acc:94.90%
Training: Epoch[083/200] Iteration[1200/2001] Loss: 0.1579 Acc:94.88%
Training: Epoch[083/200] Iteration[1500/2001] Loss: 0.1575 Acc:94.90%
Training: Epoch[083/200] Iteration[1800/2001] Loss: 0.1563 Acc:94.95%
Epoch[083/200] Train Acc: 94.94% Valid Acc:93.34% Train loss:0.1567 Valid loss:0.2238 Train fpr:1.34% Valid fpr:3.14% Train AUC:99.80% Valid AUC:99.64% LR:0.1
Training: Epoch[084/200] Iteration[300/2001] Loss: 0.1498 Acc:95.28%
Training: Epoch[084/200] Iteration[600/2001] Loss: 0.1540 Acc:95.08%
Training: Epoch[084/200] Iteration[900/2001] Loss: 0.1536 Acc:95.18%
Training: Epoch[084/200] Iteration[1200/2001] Loss: 0.1546 Acc:95.11%
Training: Epoch[084/200] Iteration[1500/2001] Loss: 0.1555 Acc:95.09%
Training: Epoch[084/200] Iteration[1800/2001] Loss: 0.1549 Acc:95.13%
Epoch[084/200] Train Acc: 95.11% Valid Acc:93.70% Train loss:0.1549 Valid loss:0.1981 Train fpr:1.21% Valid fpr:0.71% Train AUC:99.79% Valid AUC:99.87% LR:0.1
Training: Epoch[085/200] Iteration[300/2001] Loss: 0.1638 Acc:94.58%
Training: Epoch[085/200] Iteration[600/2001] Loss: 0.1632 Acc:94.77%
Training: Epoch[085/200] Iteration[900/2001] Loss: 0.1598 Acc:94.87%
Training: Epoch[085/200] Iteration[1200/2001] Loss: 0.1575 Acc:94.95%
Training: Epoch[085/200] Iteration[1500/2001] Loss: 0.1592 Acc:94.91%
Training: Epoch[085/200] Iteration[1800/2001] Loss: 0.1589 Acc:94.92%
Epoch[085/200] Train Acc: 94.94% Valid Acc:94.10% Train loss:0.1582 Valid loss:0.1807 Train fpr:1.40% Valid fpr:0.91% Train AUC:99.80% Valid AUC:99.84% LR:0.1
Training: Epoch[086/200] Iteration[300/2001] Loss: 0.1483 Acc:95.39%
Training: Epoch[086/200] Iteration[600/2001] Loss: 0.1453 Acc:95.44%
Training: Epoch[086/200] Iteration[900/2001] Loss: 0.1480 Acc:95.36%
Training: Epoch[086/200] Iteration[1200/2001] Loss: 0.1502 Acc:95.24%
Training: Epoch[086/200] Iteration[1500/2001] Loss: 0.1502 Acc:95.25%
Training: Epoch[086/200] Iteration[1800/2001] Loss: 0.1523 Acc:95.16%
Epoch[086/200] Train Acc: 95.08% Valid Acc:95.42% Train loss:0.1549 Valid loss:0.1566 Train fpr:1.31% Valid fpr:0.28% Train AUC:99.80% Valid AUC:99.89% LR:0.1
Training: Epoch[087/200] Iteration[300/2001] Loss: 0.1491 Acc:95.26%
Training: Epoch[087/200] Iteration[600/2001] Loss: 0.1554 Acc:95.12%
Training: Epoch[087/200] Iteration[900/2001] Loss: 0.1589 Acc:95.02%
Training: Epoch[087/200] Iteration[1200/2001] Loss: 0.1604 Acc:94.96%
Training: Epoch[087/200] Iteration[1500/2001] Loss: 0.1616 Acc:94.85%
Training: Epoch[087/200] Iteration[1800/2001] Loss: 0.1616 Acc:94.85%
Epoch[087/200] Train Acc: 94.84% Valid Acc:95.17% Train loss:0.1618 Valid loss:0.1507 Train fpr:1.70% Valid fpr:0.41% Train AUC:99.78% Valid AUC:99.86% LR:0.1
Training: Epoch[088/200] Iteration[300/2001] Loss: 0.1580 Acc:94.81%
Training: Epoch[088/200] Iteration[600/2001] Loss: 0.1532 Acc:94.96%
Training: Epoch[088/200] Iteration[900/2001] Loss: 0.1587 Acc:94.78%
Training: Epoch[088/200] Iteration[1200/2001] Loss: 0.1571 Acc:94.89%
Training: Epoch[088/200] Iteration[1500/2001] Loss: 0.1586 Acc:94.91%
Training: Epoch[088/200] Iteration[1800/2001] Loss: 0.1589 Acc:94.90%
Epoch[088/200] Train Acc: 94.97% Valid Acc:93.21% Train loss:0.1576 Valid loss:0.2056 Train fpr:1.30% Valid fpr:0.22% Train AUC:99.80% Valid AUC:99.90% LR:0.1
Training: Epoch[089/200] Iteration[300/2001] Loss: 0.1598 Acc:95.00%
Training: Epoch[089/200] Iteration[600/2001] Loss: 0.1605 Acc:95.00%
Training: Epoch[089/200] Iteration[900/2001] Loss: 0.1575 Acc:95.09%
Training: Epoch[089/200] Iteration[1200/2001] Loss: 0.1556 Acc:95.11%
Training: Epoch[089/200] Iteration[1500/2001] Loss: 0.1589 Acc:94.99%
Training: Epoch[089/200] Iteration[1800/2001] Loss: 0.1576 Acc:95.03%
Epoch[089/200] Train Acc: 95.08% Valid Acc:95.51% Train loss:0.1555 Valid loss:0.1492 Train fpr:1.35% Valid fpr:0.49% Train AUC:99.81% Valid AUC:99.87% LR:0.1
Training: Epoch[090/200] Iteration[300/2001] Loss: 0.1470 Acc:95.35%
Training: Epoch[090/200] Iteration[600/2001] Loss: 0.1493 Acc:95.28%
Training: Epoch[090/200] Iteration[900/2001] Loss: 0.1502 Acc:95.24%
Training: Epoch[090/200] Iteration[1200/2001] Loss: 0.1512 Acc:95.20%
Training: Epoch[090/200] Iteration[1500/2001] Loss: 0.1516 Acc:95.23%
Training: Epoch[090/200] Iteration[1800/2001] Loss: 0.1550 Acc:95.11%
Epoch[090/200] Train Acc: 95.04% Valid Acc:94.71% Train loss:0.1561 Valid loss:0.1755 Train fpr:1.29% Valid fpr:1.97% Train AUC:99.80% Valid AUC:99.80% LR:0.1
Training: Epoch[091/200] Iteration[300/2001] Loss: 0.1478 Acc:95.20%
Training: Epoch[091/200] Iteration[600/2001] Loss: 0.1497 Acc:95.17%
Training: Epoch[091/200] Iteration[900/2001] Loss: 0.1556 Acc:95.04%
Training: Epoch[091/200] Iteration[1200/2001] Loss: 0.1581 Acc:94.94%
Training: Epoch[091/200] Iteration[1500/2001] Loss: 0.1558 Acc:94.97%
Training: Epoch[091/200] Iteration[1800/2001] Loss: 0.1569 Acc:94.98%
Epoch[091/200] Train Acc: 94.96% Valid Acc:95.72% Train loss:0.1577 Valid loss:0.1344 Train fpr:1.34% Valid fpr:0.24% Train AUC:99.79% Valid AUC:99.89% LR:0.1
Training: Epoch[092/200] Iteration[300/2001] Loss: 0.1574 Acc:95.18%
Training: Epoch[092/200] Iteration[600/2001] Loss: 0.1518 Acc:95.23%
Training: Epoch[092/200] Iteration[900/2001] Loss: 0.1596 Acc:95.03%
Training: Epoch[092/200] Iteration[1200/2001] Loss: 0.1606 Acc:95.01%
Training: Epoch[092/200] Iteration[1500/2001] Loss: 0.1588 Acc:95.03%
Training: Epoch[092/200] Iteration[1800/2001] Loss: 0.1588 Acc:95.05%
Epoch[092/200] Train Acc: 95.05% Valid Acc:95.22% Train loss:0.1589 Valid loss:0.1576 Train fpr:1.36% Valid fpr:0.43% Train AUC:99.78% Valid AUC:99.89% LR:0.1
=>Best1 model updated
Training: Epoch[093/200] Iteration[300/2001] Loss: 0.1641 Acc:94.95%
Training: Epoch[093/200] Iteration[600/2001] Loss: 0.1615 Acc:95.03%
Training: Epoch[093/200] Iteration[900/2001] Loss: 0.1606 Acc:95.00%
Training: Epoch[093/200] Iteration[1200/2001] Loss: 0.1613 Acc:94.95%
Training: Epoch[093/200] Iteration[1500/2001] Loss: 0.1620 Acc:94.87%
Training: Epoch[093/200] Iteration[1800/2001] Loss: 0.1596 Acc:94.95%
Epoch[093/200] Train Acc: 94.94% Valid Acc:94.50% Train loss:0.1596 Valid loss:0.1710 Train fpr:1.51% Valid fpr:0.65% Train AUC:99.78% Valid AUC:99.89% LR:0.1
=>Best1 model updated
Training: Epoch[094/200] Iteration[300/2001] Loss: 0.1503 Acc:95.11%
Training: Epoch[094/200] Iteration[600/2001] Loss: 0.1499 Acc:95.18%
Training: Epoch[094/200] Iteration[900/2001] Loss: 0.1574 Acc:94.90%
Training: Epoch[094/200] Iteration[1200/2001] Loss: 0.1590 Acc:94.91%
Training: Epoch[094/200] Iteration[1500/2001] Loss: 0.1598 Acc:94.90%
Training: Epoch[094/200] Iteration[1800/2001] Loss: 0.1586 Acc:94.93%
Epoch[094/200] Train Acc: 94.95% Valid Acc:95.53% Train loss:0.1584 Valid loss:0.1439 Train fpr:1.29% Valid fpr:0.12% Train AUC:99.79% Valid AUC:99.90% LR:0.1
=>Best1 model updated
Training: Epoch[095/200] Iteration[300/2001] Loss: 0.1418 Acc:95.50%
Training: Epoch[095/200] Iteration[600/2001] Loss: 0.1552 Acc:94.95%
Training: Epoch[095/200] Iteration[900/2001] Loss: 0.1595 Acc:94.85%
Training: Epoch[095/200] Iteration[1200/2001] Loss: 0.1596 Acc:94.83%
Training: Epoch[095/200] Iteration[1500/2001] Loss: 0.1590 Acc:94.89%
Training: Epoch[095/200] Iteration[1800/2001] Loss: 0.1582 Acc:94.91%
Epoch[095/200] Train Acc: 94.96% Valid Acc:93.84% Train loss:0.1561 Valid loss:0.1924 Train fpr:1.39% Valid fpr:0.26% Train AUC:99.80% Valid AUC:99.88% LR:0.1
Training: Epoch[096/200] Iteration[300/2001] Loss: 0.1529 Acc:95.05%
Training: Epoch[096/200] Iteration[600/2001] Loss: 0.1564 Acc:95.08%
Training: Epoch[096/200] Iteration[900/2001] Loss: 0.1556 Acc:95.09%
Training: Epoch[096/200] Iteration[1200/2001] Loss: 0.1564 Acc:95.10%
Training: Epoch[096/200] Iteration[1500/2001] Loss: 0.1579 Acc:95.03%
Training: Epoch[096/200] Iteration[1800/2001] Loss: 0.1599 Acc:94.95%
Epoch[096/200] Train Acc: 94.93% Valid Acc:96.03% Train loss:0.1603 Valid loss:0.1321 Train fpr:1.69% Valid fpr:0.12% Train AUC:99.80% Valid AUC:99.91% LR:0.1
=>Best1 model updated
Training: Epoch[097/200] Iteration[300/2001] Loss: 0.1499 Acc:95.21%
Training: Epoch[097/200] Iteration[600/2001] Loss: 0.1546 Acc:95.00%
Training: Epoch[097/200] Iteration[900/2001] Loss: 0.1540 Acc:95.01%
Training: Epoch[097/200] Iteration[1200/2001] Loss: 0.1529 Acc:95.05%
Training: Epoch[097/200] Iteration[1500/2001] Loss: 0.1540 Acc:95.04%
Training: Epoch[097/200] Iteration[1800/2001] Loss: 0.1545 Acc:95.04%
Epoch[097/200] Train Acc: 94.99% Valid Acc:94.81% Train loss:0.1559 Valid loss:0.2017 Train fpr:1.24% Valid fpr:0.93% Train AUC:99.81% Valid AUC:99.84% LR:0.1
Training: Epoch[098/200] Iteration[300/2001] Loss: 0.1441 Acc:95.30%
Training: Epoch[098/200] Iteration[600/2001] Loss: 0.1514 Acc:95.10%
Training: Epoch[098/200] Iteration[900/2001] Loss: 0.1494 Acc:95.25%
Training: Epoch[098/200] Iteration[1200/2001] Loss: 0.1533 Acc:95.14%
Training: Epoch[098/200] Iteration[1500/2001] Loss: 0.1565 Acc:95.06%
Training: Epoch[098/200] Iteration[1800/2001] Loss: 0.1579 Acc:94.99%
Epoch[098/200] Train Acc: 94.91% Valid Acc:95.57% Train loss:0.1600 Valid loss:0.1366 Train fpr:1.34% Valid fpr:0.22% Train AUC:99.80% Valid AUC:99.88% LR:0.1
Training: Epoch[099/200] Iteration[300/2001] Loss: 0.1499 Acc:95.49%
Training: Epoch[099/200] Iteration[600/2001] Loss: 0.1533 Acc:95.18%
Training: Epoch[099/200] Iteration[900/2001] Loss: 0.1527 Acc:95.26%
Training: Epoch[099/200] Iteration[1200/2001] Loss: 0.1527 Acc:95.24%
Training: Epoch[099/200] Iteration[1500/2001] Loss: 0.1524 Acc:95.24%
Training: Epoch[099/200] Iteration[1800/2001] Loss: 0.1525 Acc:95.22%
Epoch[099/200] Train Acc: 95.17% Valid Acc:95.67% Train loss:0.1540 Valid loss:0.1315 Train fpr:1.25% Valid fpr:0.26% Train AUC:99.80% Valid AUC:99.88% LR:0.1
Training: Epoch[100/200] Iteration[300/2001] Loss: 0.1541 Acc:95.19%
Training: Epoch[100/200] Iteration[600/2001] Loss: 0.1561 Acc:95.14%
Training: Epoch[100/200] Iteration[900/2001] Loss: 0.1581 Acc:95.03%
Training: Epoch[100/200] Iteration[1200/2001] Loss: 0.1567 Acc:95.07%
Training: Epoch[100/200] Iteration[1500/2001] Loss: 0.1570 Acc:95.04%
Training: Epoch[100/200] Iteration[1800/2001] Loss: 0.1562 Acc:95.06%
Epoch[100/200] Train Acc: 95.04% Valid Acc:95.76% Train loss:0.1573 Valid loss:0.1452 Train fpr:1.42% Valid fpr:0.28% Train AUC:99.79% Valid AUC:99.85% LR:0.1
Training: Epoch[101/200] Iteration[300/2001] Loss: 0.1290 Acc:95.86%
Training: Epoch[101/200] Iteration[600/2001] Loss: 0.1257 Acc:95.98%
Training: Epoch[101/200] Iteration[900/2001] Loss: 0.1214 Acc:96.06%
Training: Epoch[101/200] Iteration[1200/2001] Loss: 0.1188 Acc:96.14%
Training: Epoch[101/200] Iteration[1500/2001] Loss: 0.1148 Acc:96.25%
Training: Epoch[101/200] Iteration[1800/2001] Loss: 0.1151 Acc:96.27%
Epoch[101/200] Train Acc: 96.31% Valid Acc:97.06% Train loss:0.1139 Valid loss:0.1008 Train fpr:0.19% Valid fpr:0.02% Train AUC:99.88% Valid AUC:99.93% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[102/200] Iteration[300/2001] Loss: 0.1056 Acc:96.50%
Training: Epoch[102/200] Iteration[600/2001] Loss: 0.1055 Acc:96.54%
Training: Epoch[102/200] Iteration[900/2001] Loss: 0.1058 Acc:96.54%
Training: Epoch[102/200] Iteration[1200/2001] Loss: 0.1061 Acc:96.53%
Training: Epoch[102/200] Iteration[1500/2001] Loss: 0.1059 Acc:96.55%
Training: Epoch[102/200] Iteration[1800/2001] Loss: 0.1055 Acc:96.55%
Epoch[102/200] Train Acc: 96.54% Valid Acc:97.12% Train loss:0.1059 Valid loss:0.0946 Train fpr:0.10% Valid fpr:0.02% Train AUC:99.90% Valid AUC:99.94% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[103/200] Iteration[300/2001] Loss: 0.1059 Acc:96.36%
Training: Epoch[103/200] Iteration[600/2001] Loss: 0.1012 Acc:96.62%
Training: Epoch[103/200] Iteration[900/2001] Loss: 0.1009 Acc:96.66%
Training: Epoch[103/200] Iteration[1200/2001] Loss: 0.1010 Acc:96.66%
Training: Epoch[103/200] Iteration[1500/2001] Loss: 0.1022 Acc:96.64%
Training: Epoch[103/200] Iteration[1800/2001] Loss: 0.1020 Acc:96.64%
Epoch[103/200] Train Acc: 96.64% Valid Acc:97.23% Train loss:0.1017 Valid loss:0.0924 Train fpr:0.09% Valid fpr:0.02% Train AUC:99.90% Valid AUC:99.94% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[104/200] Iteration[300/2001] Loss: 0.0974 Acc:96.77%
Training: Epoch[104/200] Iteration[600/2001] Loss: 0.1002 Acc:96.74%
Training: Epoch[104/200] Iteration[900/2001] Loss: 0.1012 Acc:96.69%
Training: Epoch[104/200] Iteration[1200/2001] Loss: 0.0977 Acc:96.78%
Training: Epoch[104/200] Iteration[1500/2001] Loss: 0.0975 Acc:96.77%
Training: Epoch[104/200] Iteration[1800/2001] Loss: 0.0970 Acc:96.79%
Epoch[104/200] Train Acc: 96.78% Valid Acc:97.07% Train loss:0.0978 Valid loss:0.0948 Train fpr:0.08% Valid fpr:0.00% Train AUC:99.91% Valid AUC:99.95% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[105/200] Iteration[300/2001] Loss: 0.0890 Acc:97.08%
Training: Epoch[105/200] Iteration[600/2001] Loss: 0.0944 Acc:96.90%
Training: Epoch[105/200] Iteration[900/2001] Loss: 0.0937 Acc:96.93%
Training: Epoch[105/200] Iteration[1200/2001] Loss: 0.0959 Acc:96.85%
Training: Epoch[105/200] Iteration[1500/2001] Loss: 0.0959 Acc:96.85%
Training: Epoch[105/200] Iteration[1800/2001] Loss: 0.0968 Acc:96.84%
Epoch[105/200] Train Acc: 96.79% Valid Acc:97.04% Train loss:0.0975 Valid loss:0.0954 Train fpr:0.07% Valid fpr:0.00% Train AUC:99.92% Valid AUC:99.96% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[106/200] Iteration[300/2001] Loss: 0.0900 Acc:97.03%
Training: Epoch[106/200] Iteration[600/2001] Loss: 0.0867 Acc:97.17%
Training: Epoch[106/200] Iteration[900/2001] Loss: 0.0906 Acc:97.08%
Training: Epoch[106/200] Iteration[1200/2001] Loss: 0.0929 Acc:97.02%
Training: Epoch[106/200] Iteration[1500/2001] Loss: 0.0927 Acc:97.01%
Training: Epoch[106/200] Iteration[1800/2001] Loss: 0.0932 Acc:96.97%
Epoch[106/200] Train Acc: 96.92% Valid Acc:97.03% Train loss:0.0941 Valid loss:0.0943 Train fpr:0.04% Valid fpr:0.04% Train AUC:99.93% Valid AUC:99.96% LR:0.010000000000000002
=>Best2 model updated
Training: Epoch[107/200] Iteration[300/2001] Loss: 0.0899 Acc:97.12%
Training: Epoch[107/200] Iteration[600/2001] Loss: 0.0869 Acc:97.21%
Training: Epoch[107/200] Iteration[900/2001] Loss: 0.0899 Acc:97.09%
Training: Epoch[107/200] Iteration[1200/2001] Loss: 0.0924 Acc:96.98%
Training: Epoch[107/200] Iteration[1500/2001] Loss: 0.0924 Acc:96.96%
Training: Epoch[107/200] Iteration[1800/2001] Loss: 0.0923 Acc:96.96%
Epoch[107/200] Train Acc: 96.91% Valid Acc:97.27% Train loss:0.0934 Valid loss:0.0880 Train fpr:0.03% Valid fpr:0.00% Train AUC:99.93% Valid AUC:99.97% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[108/200] Iteration[300/2001] Loss: 0.0860 Acc:97.20%
Training: Epoch[108/200] Iteration[600/2001] Loss: 0.0928 Acc:96.98%
Training: Epoch[108/200] Iteration[900/2001] Loss: 0.0929 Acc:96.99%
Training: Epoch[108/200] Iteration[1200/2001] Loss: 0.0922 Acc:96.99%
Training: Epoch[108/200] Iteration[1500/2001] Loss: 0.0916 Acc:96.96%
Training: Epoch[108/200] Iteration[1800/2001] Loss: 0.0912 Acc:96.97%
Epoch[108/200] Train Acc: 96.95% Valid Acc:97.28% Train loss:0.0920 Valid loss:0.0875 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.93% Valid AUC:99.97% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[109/200] Iteration[300/2001] Loss: 0.0885 Acc:97.18%
Training: Epoch[109/200] Iteration[600/2001] Loss: 0.0923 Acc:96.94%
Training: Epoch[109/200] Iteration[900/2001] Loss: 0.0929 Acc:96.93%
Training: Epoch[109/200] Iteration[1200/2001] Loss: 0.0933 Acc:96.90%
Training: Epoch[109/200] Iteration[1500/2001] Loss: 0.0919 Acc:96.96%
Training: Epoch[109/200] Iteration[1800/2001] Loss: 0.0910 Acc:97.00%
Epoch[109/200] Train Acc: 97.01% Valid Acc:97.13% Train loss:0.0907 Valid loss:0.0917 Train fpr:0.03% Valid fpr:0.02% Train AUC:99.94% Valid AUC:99.97% LR:0.010000000000000002
=>Best2 model updated
Training: Epoch[110/200] Iteration[300/2001] Loss: 0.0922 Acc:96.88%
Training: Epoch[110/200] Iteration[600/2001] Loss: 0.0941 Acc:96.88%
Training: Epoch[110/200] Iteration[900/2001] Loss: 0.0906 Acc:97.04%
Training: Epoch[110/200] Iteration[1200/2001] Loss: 0.0906 Acc:96.99%
Training: Epoch[110/200] Iteration[1500/2001] Loss: 0.0923 Acc:96.95%
Training: Epoch[110/200] Iteration[1800/2001] Loss: 0.0915 Acc:96.99%
Epoch[110/200] Train Acc: 97.03% Valid Acc:97.37% Train loss:0.0905 Valid loss:0.0923 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.95% Valid AUC:99.96% LR:0.010000000000000002
Training: Epoch[111/200] Iteration[300/2001] Loss: 0.0843 Acc:97.19%
Training: Epoch[111/200] Iteration[600/2001] Loss: 0.0834 Acc:97.22%
Training: Epoch[111/200] Iteration[900/2001] Loss: 0.0852 Acc:97.16%
Training: Epoch[111/200] Iteration[1200/2001] Loss: 0.0885 Acc:97.10%
Training: Epoch[111/200] Iteration[1500/2001] Loss: 0.0884 Acc:97.08%
Training: Epoch[111/200] Iteration[1800/2001] Loss: 0.0896 Acc:97.03%
Epoch[111/200] Train Acc: 97.04% Valid Acc:97.26% Train loss:0.0893 Valid loss:0.0921 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.95% Valid AUC:99.97% LR:0.010000000000000002
Training: Epoch[112/200] Iteration[300/2001] Loss: 0.0909 Acc:97.12%
Training: Epoch[112/200] Iteration[600/2001] Loss: 0.0912 Acc:96.98%
Training: Epoch[112/200] Iteration[900/2001] Loss: 0.0885 Acc:97.07%
Training: Epoch[112/200] Iteration[1200/2001] Loss: 0.0879 Acc:97.04%
Training: Epoch[112/200] Iteration[1500/2001] Loss: 0.0884 Acc:97.01%
Training: Epoch[112/200] Iteration[1800/2001] Loss: 0.0886 Acc:97.01%
Epoch[112/200] Train Acc: 97.02% Valid Acc:97.12% Train loss:0.0886 Valid loss:0.0919 Train fpr:0.03% Valid fpr:0.00% Train AUC:99.95% Valid AUC:99.98% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[113/200] Iteration[300/2001] Loss: 0.0837 Acc:97.21%
Training: Epoch[113/200] Iteration[600/2001] Loss: 0.0857 Acc:97.10%
Training: Epoch[113/200] Iteration[900/2001] Loss: 0.0847 Acc:97.14%
Training: Epoch[113/200] Iteration[1200/2001] Loss: 0.0852 Acc:97.15%
Training: Epoch[113/200] Iteration[1500/2001] Loss: 0.0855 Acc:97.14%
Training: Epoch[113/200] Iteration[1800/2001] Loss: 0.0866 Acc:97.11%
Epoch[113/200] Train Acc: 97.08% Valid Acc:97.31% Train loss:0.0868 Valid loss:0.0878 Train fpr:0.04% Valid fpr:0.00% Train AUC:99.95% Valid AUC:99.98% LR:0.010000000000000002
=>Best2 model updated
Training: Epoch[114/200] Iteration[300/2001] Loss: 0.0916 Acc:96.97%
Training: Epoch[114/200] Iteration[600/2001] Loss: 0.0900 Acc:97.01%
Training: Epoch[114/200] Iteration[900/2001] Loss: 0.0867 Acc:97.05%
Training: Epoch[114/200] Iteration[1200/2001] Loss: 0.0851 Acc:97.13%
Training: Epoch[114/200] Iteration[1500/2001] Loss: 0.0859 Acc:97.10%
Training: Epoch[114/200] Iteration[1800/2001] Loss: 0.0862 Acc:97.08%
Epoch[114/200] Train Acc: 97.08% Valid Acc:97.16% Train loss:0.0866 Valid loss:0.0910 Train fpr:0.02% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.98% LR:0.010000000000000002
=>Best2 model updated
Training: Epoch[115/200] Iteration[300/2001] Loss: 0.0839 Acc:97.12%
Training: Epoch[115/200] Iteration[600/2001] Loss: 0.0856 Acc:96.99%
Training: Epoch[115/200] Iteration[900/2001] Loss: 0.0863 Acc:97.05%
Training: Epoch[115/200] Iteration[1200/2001] Loss: 0.0877 Acc:96.98%
Training: Epoch[115/200] Iteration[1500/2001] Loss: 0.0868 Acc:97.04%
Training: Epoch[115/200] Iteration[1800/2001] Loss: 0.0850 Acc:97.11%
Epoch[115/200] Train Acc: 97.10% Valid Acc:97.21% Train loss:0.0850 Valid loss:0.0910 Train fpr:0.04% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.97% LR:0.010000000000000002
Training: Epoch[116/200] Iteration[300/2001] Loss: 0.0830 Acc:97.17%
Training: Epoch[116/200] Iteration[600/2001] Loss: 0.0861 Acc:97.11%
Training: Epoch[116/200] Iteration[900/2001] Loss: 0.0831 Acc:97.22%
Training: Epoch[116/200] Iteration[1200/2001] Loss: 0.0844 Acc:97.18%
Training: Epoch[116/200] Iteration[1500/2001] Loss: 0.0842 Acc:97.19%
Training: Epoch[116/200] Iteration[1800/2001] Loss: 0.0854 Acc:97.12%
Epoch[116/200] Train Acc: 97.10% Valid Acc:97.22% Train loss:0.0857 Valid loss:0.0912 Train fpr:0.04% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.98% LR:0.010000000000000002
=>Best3 model updated
Training: Epoch[117/200] Iteration[300/2001] Loss: 0.0873 Acc:97.14%
Training: Epoch[117/200] Iteration[600/2001] Loss: 0.0839 Acc:97.17%
Training: Epoch[117/200] Iteration[900/2001] Loss: 0.0839 Acc:97.24%
Training: Epoch[117/200] Iteration[1200/2001] Loss: 0.0827 Acc:97.25%
Training: Epoch[117/200] Iteration[1500/2001] Loss: 0.0828 Acc:97.22%
Training: Epoch[117/200] Iteration[1800/2001] Loss: 0.0847 Acc:97.12%
Epoch[117/200] Train Acc: 97.11% Valid Acc:97.37% Train loss:0.0848 Valid loss:0.0875 Train fpr:0.03% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.98% LR:0.010000000000000002
=>Best3 model updated
Training: Epoch[118/200] Iteration[300/2001] Loss: 0.0848 Acc:97.06%
Training: Epoch[118/200] Iteration[600/2001] Loss: 0.0858 Acc:97.06%
Training: Epoch[118/200] Iteration[900/2001] Loss: 0.0876 Acc:96.98%
Training: Epoch[118/200] Iteration[1200/2001] Loss: 0.0863 Acc:96.99%
Training: Epoch[118/200] Iteration[1500/2001] Loss: 0.0863 Acc:97.02%
Training: Epoch[118/200] Iteration[1800/2001] Loss: 0.0866 Acc:97.04%
Epoch[118/200] Train Acc: 97.05% Valid Acc:97.23% Train loss:0.0864 Valid loss:0.0908 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[119/200] Iteration[300/2001] Loss: 0.0818 Acc:97.22%
Training: Epoch[119/200] Iteration[600/2001] Loss: 0.0834 Acc:97.21%
Training: Epoch[119/200] Iteration[900/2001] Loss: 0.0875 Acc:97.09%
Training: Epoch[119/200] Iteration[1200/2001] Loss: 0.0875 Acc:97.07%
Training: Epoch[119/200] Iteration[1500/2001] Loss: 0.0852 Acc:97.12%
Training: Epoch[119/200] Iteration[1800/2001] Loss: 0.0847 Acc:97.16%
Epoch[119/200] Train Acc: 97.17% Valid Acc:97.03% Train loss:0.0843 Valid loss:0.0932 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.97% LR:0.010000000000000002
Training: Epoch[120/200] Iteration[300/2001] Loss: 0.0810 Acc:97.44%
Training: Epoch[120/200] Iteration[600/2001] Loss: 0.0810 Acc:97.31%
Training: Epoch[120/200] Iteration[900/2001] Loss: 0.0830 Acc:97.24%
Training: Epoch[120/200] Iteration[1200/2001] Loss: 0.0831 Acc:97.22%
Training: Epoch[120/200] Iteration[1500/2001] Loss: 0.0852 Acc:97.16%
Training: Epoch[120/200] Iteration[1800/2001] Loss: 0.0842 Acc:97.17%
Epoch[120/200] Train Acc: 97.19% Valid Acc:97.23% Train loss:0.0842 Valid loss:0.0924 Train fpr:0.04% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.96% LR:0.010000000000000002
Training: Epoch[121/200] Iteration[300/2001] Loss: 0.0903 Acc:97.11%
Training: Epoch[121/200] Iteration[600/2001] Loss: 0.0872 Acc:97.22%
Training: Epoch[121/200] Iteration[900/2001] Loss: 0.0854 Acc:97.20%
Training: Epoch[121/200] Iteration[1200/2001] Loss: 0.0858 Acc:97.14%
Training: Epoch[121/200] Iteration[1500/2001] Loss: 0.0860 Acc:97.09%
Training: Epoch[121/200] Iteration[1800/2001] Loss: 0.0862 Acc:97.10%
Epoch[121/200] Train Acc: 97.10% Valid Acc:97.24% Train loss:0.0863 Valid loss:0.0907 Train fpr:0.06% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[122/200] Iteration[300/2001] Loss: 0.0736 Acc:97.44%
Training: Epoch[122/200] Iteration[600/2001] Loss: 0.0819 Acc:97.18%
Training: Epoch[122/200] Iteration[900/2001] Loss: 0.0848 Acc:97.07%
Training: Epoch[122/200] Iteration[1200/2001] Loss: 0.0850 Acc:97.08%
Training: Epoch[122/200] Iteration[1500/2001] Loss: 0.0850 Acc:97.10%
Training: Epoch[122/200] Iteration[1800/2001] Loss: 0.0859 Acc:97.06%
Epoch[122/200] Train Acc: 97.09% Valid Acc:97.32% Train loss:0.0855 Valid loss:0.0898 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.97% LR:0.010000000000000002
Training: Epoch[123/200] Iteration[300/2001] Loss: 0.0826 Acc:97.27%
Training: Epoch[123/200] Iteration[600/2001] Loss: 0.0830 Acc:97.22%
Training: Epoch[123/200] Iteration[900/2001] Loss: 0.0825 Acc:97.27%
Training: Epoch[123/200] Iteration[1200/2001] Loss: 0.0826 Acc:97.25%
Training: Epoch[123/200] Iteration[1500/2001] Loss: 0.0837 Acc:97.17%
Training: Epoch[123/200] Iteration[1800/2001] Loss: 0.0838 Acc:97.21%
Epoch[123/200] Train Acc: 97.17% Valid Acc:96.84% Train loss:0.0841 Valid loss:0.0930 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[124/200] Iteration[300/2001] Loss: 0.0785 Acc:97.36%
Training: Epoch[124/200] Iteration[600/2001] Loss: 0.0832 Acc:97.16%
Training: Epoch[124/200] Iteration[900/2001] Loss: 0.0852 Acc:97.14%
Training: Epoch[124/200] Iteration[1200/2001] Loss: 0.0853 Acc:97.14%
Training: Epoch[124/200] Iteration[1500/2001] Loss: 0.0837 Acc:97.16%
Training: Epoch[124/200] Iteration[1800/2001] Loss: 0.0851 Acc:97.11%
Epoch[124/200] Train Acc: 97.11% Valid Acc:97.07% Train loss:0.0856 Valid loss:0.0997 Train fpr:0.05% Valid fpr:0.02% Train AUC:99.96% Valid AUC:99.96% LR:0.010000000000000002
Training: Epoch[125/200] Iteration[300/2001] Loss: 0.0828 Acc:97.18%
Training: Epoch[125/200] Iteration[600/2001] Loss: 0.0809 Acc:97.20%
Training: Epoch[125/200] Iteration[900/2001] Loss: 0.0820 Acc:97.19%
Training: Epoch[125/200] Iteration[1200/2001] Loss: 0.0837 Acc:97.13%
Training: Epoch[125/200] Iteration[1500/2001] Loss: 0.0840 Acc:97.16%
Training: Epoch[125/200] Iteration[1800/2001] Loss: 0.0845 Acc:97.16%
Epoch[125/200] Train Acc: 97.18% Valid Acc:97.38% Train loss:0.0842 Valid loss:0.0865 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.98% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[126/200] Iteration[300/2001] Loss: 0.0779 Acc:97.34%
Training: Epoch[126/200] Iteration[600/2001] Loss: 0.0803 Acc:97.30%
Training: Epoch[126/200] Iteration[900/2001] Loss: 0.0793 Acc:97.30%
Training: Epoch[126/200] Iteration[1200/2001] Loss: 0.0797 Acc:97.28%
Training: Epoch[126/200] Iteration[1500/2001] Loss: 0.0810 Acc:97.25%
Training: Epoch[126/200] Iteration[1800/2001] Loss: 0.0813 Acc:97.24%
Epoch[126/200] Train Acc: 97.18% Valid Acc:97.04% Train loss:0.0832 Valid loss:0.0901 Train fpr:0.03% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.96% LR:0.010000000000000002
Training: Epoch[127/200] Iteration[300/2001] Loss: 0.0782 Acc:97.23%
Training: Epoch[127/200] Iteration[600/2001] Loss: 0.0763 Acc:97.31%
Training: Epoch[127/200] Iteration[900/2001] Loss: 0.0793 Acc:97.15%
Training: Epoch[127/200] Iteration[1200/2001] Loss: 0.0807 Acc:97.14%
Training: Epoch[127/200] Iteration[1500/2001] Loss: 0.0808 Acc:97.16%
Training: Epoch[127/200] Iteration[1800/2001] Loss: 0.0815 Acc:97.16%
Epoch[127/200] Train Acc: 97.15% Valid Acc:97.19% Train loss:0.0825 Valid loss:0.0939 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.96% LR:0.010000000000000002
Training: Epoch[128/200] Iteration[300/2001] Loss: 0.0783 Acc:97.33%
Training: Epoch[128/200] Iteration[600/2001] Loss: 0.0804 Acc:97.24%
Training: Epoch[128/200] Iteration[900/2001] Loss: 0.0838 Acc:97.17%
Training: Epoch[128/200] Iteration[1200/2001] Loss: 0.0831 Acc:97.17%
Training: Epoch[128/200] Iteration[1500/2001] Loss: 0.0840 Acc:97.17%
Training: Epoch[128/200] Iteration[1800/2001] Loss: 0.0845 Acc:97.15%
Epoch[128/200] Train Acc: 97.14% Valid Acc:97.26% Train loss:0.0843 Valid loss:0.0863 Train fpr:0.06% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[129/200] Iteration[300/2001] Loss: 0.0752 Acc:97.40%
Training: Epoch[129/200] Iteration[600/2001] Loss: 0.0838 Acc:97.04%
Training: Epoch[129/200] Iteration[900/2001] Loss: 0.0829 Acc:97.14%
Training: Epoch[129/200] Iteration[1200/2001] Loss: 0.0825 Acc:97.18%
Training: Epoch[129/200] Iteration[1500/2001] Loss: 0.0818 Acc:97.21%
Training: Epoch[129/200] Iteration[1800/2001] Loss: 0.0816 Acc:97.21%
Epoch[129/200] Train Acc: 97.17% Valid Acc:97.03% Train loss:0.0831 Valid loss:0.0976 Train fpr:0.03% Valid fpr:0.02% Train AUC:99.97% Valid AUC:99.95% LR:0.010000000000000002
Training: Epoch[130/200] Iteration[300/2001] Loss: 0.0835 Acc:97.15%
Training: Epoch[130/200] Iteration[600/2001] Loss: 0.0834 Acc:97.22%
Training: Epoch[130/200] Iteration[900/2001] Loss: 0.0817 Acc:97.24%
Training: Epoch[130/200] Iteration[1200/2001] Loss: 0.0840 Acc:97.12%
Training: Epoch[130/200] Iteration[1500/2001] Loss: 0.0840 Acc:97.12%
Training: Epoch[130/200] Iteration[1800/2001] Loss: 0.0830 Acc:97.17%
Epoch[130/200] Train Acc: 97.16% Valid Acc:97.19% Train loss:0.0834 Valid loss:0.0897 Train fpr:0.03% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.98% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[131/200] Iteration[300/2001] Loss: 0.0843 Acc:97.04%
Training: Epoch[131/200] Iteration[600/2001] Loss: 0.0839 Acc:97.15%
Training: Epoch[131/200] Iteration[900/2001] Loss: 0.0844 Acc:97.15%
Training: Epoch[131/200] Iteration[1200/2001] Loss: 0.0843 Acc:97.15%
Training: Epoch[131/200] Iteration[1500/2001] Loss: 0.0845 Acc:97.11%
Training: Epoch[131/200] Iteration[1800/2001] Loss: 0.0851 Acc:97.10%
Epoch[131/200] Train Acc: 97.11% Valid Acc:97.12% Train loss:0.0842 Valid loss:0.0907 Train fpr:0.04% Valid fpr:0.02% Train AUC:99.97% Valid AUC:99.98% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[132/200] Iteration[300/2001] Loss: 0.0769 Acc:97.19%
Training: Epoch[132/200] Iteration[600/2001] Loss: 0.0787 Acc:97.26%
Training: Epoch[132/200] Iteration[900/2001] Loss: 0.0816 Acc:97.18%
Training: Epoch[132/200] Iteration[1200/2001] Loss: 0.0813 Acc:97.20%
Training: Epoch[132/200] Iteration[1500/2001] Loss: 0.0830 Acc:97.18%
Training: Epoch[132/200] Iteration[1800/2001] Loss: 0.0826 Acc:97.17%
Epoch[132/200] Train Acc: 97.19% Valid Acc:97.16% Train loss:0.0824 Valid loss:0.0936 Train fpr:0.03% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.97% LR:0.010000000000000002
Training: Epoch[133/200] Iteration[300/2001] Loss: 0.0770 Acc:97.56%
Training: Epoch[133/200] Iteration[600/2001] Loss: 0.0798 Acc:97.45%
Training: Epoch[133/200] Iteration[900/2001] Loss: 0.0799 Acc:97.38%
Training: Epoch[133/200] Iteration[1200/2001] Loss: 0.0810 Acc:97.32%
Training: Epoch[133/200] Iteration[1500/2001] Loss: 0.0828 Acc:97.26%
Training: Epoch[133/200] Iteration[1800/2001] Loss: 0.0835 Acc:97.26%
Epoch[133/200] Train Acc: 97.26% Valid Acc:97.16% Train loss:0.0831 Valid loss:0.0938 Train fpr:0.04% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.97% LR:0.010000000000000002
Training: Epoch[134/200] Iteration[300/2001] Loss: 0.0762 Acc:97.39%
Training: Epoch[134/200] Iteration[600/2001] Loss: 0.0802 Acc:97.19%
Training: Epoch[134/200] Iteration[900/2001] Loss: 0.0804 Acc:97.18%
Training: Epoch[134/200] Iteration[1200/2001] Loss: 0.0825 Acc:97.15%
Training: Epoch[134/200] Iteration[1500/2001] Loss: 0.0824 Acc:97.18%
Training: Epoch[134/200] Iteration[1800/2001] Loss: 0.0832 Acc:97.15%
Epoch[134/200] Train Acc: 97.18% Valid Acc:97.22% Train loss:0.0830 Valid loss:0.0859 Train fpr:0.06% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[135/200] Iteration[300/2001] Loss: 0.0799 Acc:97.02%
Training: Epoch[135/200] Iteration[600/2001] Loss: 0.0794 Acc:97.20%
Training: Epoch[135/200] Iteration[900/2001] Loss: 0.0811 Acc:97.22%
Training: Epoch[135/200] Iteration[1200/2001] Loss: 0.0808 Acc:97.28%
Training: Epoch[135/200] Iteration[1500/2001] Loss: 0.0820 Acc:97.20%
Training: Epoch[135/200] Iteration[1800/2001] Loss: 0.0821 Acc:97.19%
Epoch[135/200] Train Acc: 97.17% Valid Acc:96.92% Train loss:0.0825 Valid loss:0.1033 Train fpr:0.07% Valid fpr:0.02% Train AUC:99.96% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[136/200] Iteration[300/2001] Loss: 0.0883 Acc:96.73%
Training: Epoch[136/200] Iteration[600/2001] Loss: 0.0852 Acc:96.98%
Training: Epoch[136/200] Iteration[900/2001] Loss: 0.0853 Acc:96.98%
Training: Epoch[136/200] Iteration[1200/2001] Loss: 0.0845 Acc:97.05%
Training: Epoch[136/200] Iteration[1500/2001] Loss: 0.0848 Acc:97.03%
Training: Epoch[136/200] Iteration[1800/2001] Loss: 0.0837 Acc:97.09%
Epoch[136/200] Train Acc: 97.11% Valid Acc:97.07% Train loss:0.0831 Valid loss:0.1002 Train fpr:0.04% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.96% LR:0.010000000000000002
Training: Epoch[137/200] Iteration[300/2001] Loss: 0.0820 Acc:97.15%
Training: Epoch[137/200] Iteration[600/2001] Loss: 0.0818 Acc:97.17%
Training: Epoch[137/200] Iteration[900/2001] Loss: 0.0817 Acc:97.18%
Training: Epoch[137/200] Iteration[1200/2001] Loss: 0.0818 Acc:97.21%
Training: Epoch[137/200] Iteration[1500/2001] Loss: 0.0833 Acc:97.13%
Training: Epoch[137/200] Iteration[1800/2001] Loss: 0.0838 Acc:97.12%
Epoch[137/200] Train Acc: 97.10% Valid Acc:96.92% Train loss:0.0847 Valid loss:0.1026 Train fpr:0.05% Valid fpr:0.06% Train AUC:99.97% Valid AUC:99.96% LR:0.010000000000000002
Training: Epoch[138/200] Iteration[300/2001] Loss: 0.0748 Acc:97.31%
Training: Epoch[138/200] Iteration[600/2001] Loss: 0.0770 Acc:97.38%
Training: Epoch[138/200] Iteration[900/2001] Loss: 0.0792 Acc:97.29%
Training: Epoch[138/200] Iteration[1200/2001] Loss: 0.0798 Acc:97.27%
Training: Epoch[138/200] Iteration[1500/2001] Loss: 0.0802 Acc:97.24%
Training: Epoch[138/200] Iteration[1800/2001] Loss: 0.0798 Acc:97.27%
Epoch[138/200] Train Acc: 97.23% Valid Acc:97.01% Train loss:0.0810 Valid loss:0.0957 Train fpr:0.03% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.97% LR:0.010000000000000002
Training: Epoch[139/200] Iteration[300/2001] Loss: 0.0732 Acc:97.48%
Training: Epoch[139/200] Iteration[600/2001] Loss: 0.0810 Acc:97.17%
Training: Epoch[139/200] Iteration[900/2001] Loss: 0.0825 Acc:97.08%
Training: Epoch[139/200] Iteration[1200/2001] Loss: 0.0835 Acc:97.10%
Training: Epoch[139/200] Iteration[1500/2001] Loss: 0.0830 Acc:97.17%
Training: Epoch[139/200] Iteration[1800/2001] Loss: 0.0831 Acc:97.18%
Epoch[139/200] Train Acc: 97.19% Valid Acc:97.32% Train loss:0.0827 Valid loss:0.0887 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.96% Valid AUC:99.99% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[140/200] Iteration[300/2001] Loss: 0.0763 Acc:97.43%
Training: Epoch[140/200] Iteration[600/2001] Loss: 0.0815 Acc:97.25%
Training: Epoch[140/200] Iteration[900/2001] Loss: 0.0796 Acc:97.27%
Training: Epoch[140/200] Iteration[1200/2001] Loss: 0.0815 Acc:97.25%
Training: Epoch[140/200] Iteration[1500/2001] Loss: 0.0816 Acc:97.25%
Training: Epoch[140/200] Iteration[1800/2001] Loss: 0.0821 Acc:97.23%
Epoch[140/200] Train Acc: 97.19% Valid Acc:96.77% Train loss:0.0836 Valid loss:0.1004 Train fpr:0.05% Valid fpr:0.02% Train AUC:99.96% Valid AUC:99.99% LR:0.010000000000000002
=>Best2 model updated
Training: Epoch[141/200] Iteration[300/2001] Loss: 0.0848 Acc:97.07%
Training: Epoch[141/200] Iteration[600/2001] Loss: 0.0788 Acc:97.33%
Training: Epoch[141/200] Iteration[900/2001] Loss: 0.0808 Acc:97.28%
Training: Epoch[141/200] Iteration[1200/2001] Loss: 0.0831 Acc:97.19%
Training: Epoch[141/200] Iteration[1500/2001] Loss: 0.0829 Acc:97.20%
Training: Epoch[141/200] Iteration[1800/2001] Loss: 0.0829 Acc:97.19%
Epoch[141/200] Train Acc: 97.19% Valid Acc:97.14% Train loss:0.0828 Valid loss:0.0914 Train fpr:0.04% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.99% LR:0.010000000000000002
=>Best3 model updated
Training: Epoch[142/200] Iteration[300/2001] Loss: 0.0865 Acc:97.05%
Training: Epoch[142/200] Iteration[600/2001] Loss: 0.0883 Acc:97.02%
Training: Epoch[142/200] Iteration[900/2001] Loss: 0.0846 Acc:97.09%
Training: Epoch[142/200] Iteration[1200/2001] Loss: 0.0847 Acc:97.13%
Training: Epoch[142/200] Iteration[1500/2001] Loss: 0.0834 Acc:97.16%
Training: Epoch[142/200] Iteration[1800/2001] Loss: 0.0839 Acc:97.15%
Epoch[142/200] Train Acc: 97.15% Valid Acc:97.21% Train loss:0.0837 Valid loss:0.0888 Train fpr:0.04% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.99% LR:0.010000000000000002
=>Best3 model updated
Training: Epoch[143/200] Iteration[300/2001] Loss: 0.0805 Acc:97.40%
Training: Epoch[143/200] Iteration[600/2001] Loss: 0.0817 Acc:97.26%
Training: Epoch[143/200] Iteration[900/2001] Loss: 0.0821 Acc:97.24%
Training: Epoch[143/200] Iteration[1200/2001] Loss: 0.0834 Acc:97.19%
Training: Epoch[143/200] Iteration[1500/2001] Loss: 0.0834 Acc:97.16%
Training: Epoch[143/200] Iteration[1800/2001] Loss: 0.0821 Acc:97.19%
Epoch[143/200] Train Acc: 97.22% Valid Acc:97.11% Train loss:0.0814 Valid loss:0.0910 Train fpr:0.05% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.99% LR:0.010000000000000002
=>Best1 model updated
Training: Epoch[144/200] Iteration[300/2001] Loss: 0.0791 Acc:97.22%
Training: Epoch[144/200] Iteration[600/2001] Loss: 0.0785 Acc:97.27%
Training: Epoch[144/200] Iteration[900/2001] Loss: 0.0792 Acc:97.30%
Training: Epoch[144/200] Iteration[1200/2001] Loss: 0.0808 Acc:97.29%
Training: Epoch[144/200] Iteration[1500/2001] Loss: 0.0800 Acc:97.30%
Training: Epoch[144/200] Iteration[1800/2001] Loss: 0.0810 Acc:97.28%
Epoch[144/200] Train Acc: 97.28% Valid Acc:97.03% Train loss:0.0816 Valid loss:0.0949 Train fpr:0.06% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[145/200] Iteration[300/2001] Loss: 0.0756 Acc:97.55%
Training: Epoch[145/200] Iteration[600/2001] Loss: 0.0763 Acc:97.48%
Training: Epoch[145/200] Iteration[900/2001] Loss: 0.0781 Acc:97.36%
Training: Epoch[145/200] Iteration[1200/2001] Loss: 0.0805 Acc:97.26%
Training: Epoch[145/200] Iteration[1500/2001] Loss: 0.0822 Acc:97.22%
Training: Epoch[145/200] Iteration[1800/2001] Loss: 0.0823 Acc:97.21%
Epoch[145/200] Train Acc: 97.21% Valid Acc:96.87% Train loss:0.0822 Valid loss:0.1060 Train fpr:0.06% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[146/200] Iteration[300/2001] Loss: 0.0796 Acc:97.34%
Training: Epoch[146/200] Iteration[600/2001] Loss: 0.0802 Acc:97.32%
Training: Epoch[146/200] Iteration[900/2001] Loss: 0.0799 Acc:97.30%
Training: Epoch[146/200] Iteration[1200/2001] Loss: 0.0826 Acc:97.23%
Training: Epoch[146/200] Iteration[1500/2001] Loss: 0.0826 Acc:97.20%
Training: Epoch[146/200] Iteration[1800/2001] Loss: 0.0820 Acc:97.22%
Epoch[146/200] Train Acc: 97.21% Valid Acc:96.44% Train loss:0.0826 Valid loss:0.1175 Train fpr:0.06% Valid fpr:0.10% Train AUC:99.97% Valid AUC:99.97% LR:0.010000000000000002
Training: Epoch[147/200] Iteration[300/2001] Loss: 0.0826 Acc:97.04%
Training: Epoch[147/200] Iteration[600/2001] Loss: 0.0824 Acc:97.10%
Training: Epoch[147/200] Iteration[900/2001] Loss: 0.0827 Acc:97.11%
Training: Epoch[147/200] Iteration[1200/2001] Loss: 0.0827 Acc:97.15%
Training: Epoch[147/200] Iteration[1500/2001] Loss: 0.0839 Acc:97.11%
Training: Epoch[147/200] Iteration[1800/2001] Loss: 0.0827 Acc:97.15%
Epoch[147/200] Train Acc: 97.14% Valid Acc:97.13% Train loss:0.0828 Valid loss:0.0992 Train fpr:0.05% Valid fpr:0.02% Train AUC:99.97% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[148/200] Iteration[300/2001] Loss: 0.0816 Acc:97.26%
Training: Epoch[148/200] Iteration[600/2001] Loss: 0.0834 Acc:97.21%
Training: Epoch[148/200] Iteration[900/2001] Loss: 0.0834 Acc:97.19%
Training: Epoch[148/200] Iteration[1200/2001] Loss: 0.0829 Acc:97.22%
Training: Epoch[148/200] Iteration[1500/2001] Loss: 0.0843 Acc:97.20%
Training: Epoch[148/200] Iteration[1800/2001] Loss: 0.0832 Acc:97.22%
Epoch[148/200] Train Acc: 97.20% Valid Acc:96.87% Train loss:0.0832 Valid loss:0.0955 Train fpr:0.06% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[149/200] Iteration[300/2001] Loss: 0.0702 Acc:97.54%
Training: Epoch[149/200] Iteration[600/2001] Loss: 0.0754 Acc:97.41%
Training: Epoch[149/200] Iteration[900/2001] Loss: 0.0788 Acc:97.31%
Training: Epoch[149/200] Iteration[1200/2001] Loss: 0.0788 Acc:97.28%
Training: Epoch[149/200] Iteration[1500/2001] Loss: 0.0804 Acc:97.22%
Training: Epoch[149/200] Iteration[1800/2001] Loss: 0.0813 Acc:97.22%
Epoch[149/200] Train Acc: 97.24% Valid Acc:97.01% Train loss:0.0812 Valid loss:0.1032 Train fpr:0.05% Valid fpr:0.04% Train AUC:99.97% Valid AUC:99.98% LR:0.010000000000000002
Training: Epoch[150/200] Iteration[300/2001] Loss: 0.0808 Acc:97.29%
Training: Epoch[150/200] Iteration[600/2001] Loss: 0.0811 Acc:97.24%
Training: Epoch[150/200] Iteration[900/2001] Loss: 0.0821 Acc:97.26%
Training: Epoch[150/200] Iteration[1200/2001] Loss: 0.0824 Acc:97.27%
Training: Epoch[150/200] Iteration[1500/2001] Loss: 0.0833 Acc:97.24%
Training: Epoch[150/200] Iteration[1800/2001] Loss: 0.0828 Acc:97.24%
Epoch[150/200] Train Acc: 97.21% Valid Acc:96.92% Train loss:0.0833 Valid loss:0.0988 Train fpr:0.04% Valid fpr:0.00% Train AUC:99.97% Valid AUC:99.97% LR:0.010000000000000002
Training: Epoch[151/200] Iteration[300/2001] Loss: 0.0711 Acc:97.56%
Training: Epoch[151/200] Iteration[600/2001] Loss: 0.0706 Acc:97.55%
Training: Epoch[151/200] Iteration[900/2001] Loss: 0.0696 Acc:97.57%
Training: Epoch[151/200] Iteration[1200/2001] Loss: 0.0687 Acc:97.61%
Training: Epoch[151/200] Iteration[1500/2001] Loss: 0.0679 Acc:97.64%
Training: Epoch[151/200] Iteration[1800/2001] Loss: 0.0670 Acc:97.69%
Epoch[151/200] Train Acc: 97.69% Valid Acc:97.37% Train loss:0.0670 Valid loss:0.0816 Train fpr:0.01% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[152/200] Iteration[300/2001] Loss: 0.0612 Acc:97.90%
Training: Epoch[152/200] Iteration[600/2001] Loss: 0.0613 Acc:97.95%
Training: Epoch[152/200] Iteration[900/2001] Loss: 0.0608 Acc:97.93%
Training: Epoch[152/200] Iteration[1200/2001] Loss: 0.0598 Acc:97.96%
Training: Epoch[152/200] Iteration[1500/2001] Loss: 0.0606 Acc:97.92%
Training: Epoch[152/200] Iteration[1800/2001] Loss: 0.0606 Acc:97.91%
Epoch[152/200] Train Acc: 97.91% Valid Acc:97.46% Train loss:0.0609 Valid loss:0.0818 Train fpr:0.01% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
=>Best2 model updated
Training: Epoch[153/200] Iteration[300/2001] Loss: 0.0585 Acc:97.98%
Training: Epoch[153/200] Iteration[600/2001] Loss: 0.0605 Acc:97.92%
Training: Epoch[153/200] Iteration[900/2001] Loss: 0.0601 Acc:97.93%
Training: Epoch[153/200] Iteration[1200/2001] Loss: 0.0591 Acc:97.98%
Training: Epoch[153/200] Iteration[1500/2001] Loss: 0.0592 Acc:97.97%
Training: Epoch[153/200] Iteration[1800/2001] Loss: 0.0595 Acc:98.00%
Epoch[153/200] Train Acc: 98.00% Valid Acc:97.39% Train loss:0.0597 Valid loss:0.0821 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
=>Best3 model updated
Training: Epoch[154/200] Iteration[300/2001] Loss: 0.0517 Acc:98.10%
Training: Epoch[154/200] Iteration[600/2001] Loss: 0.0529 Acc:98.13%
Training: Epoch[154/200] Iteration[900/2001] Loss: 0.0540 Acc:98.12%
Training: Epoch[154/200] Iteration[1200/2001] Loss: 0.0549 Acc:98.10%
Training: Epoch[154/200] Iteration[1500/2001] Loss: 0.0562 Acc:98.05%
Training: Epoch[154/200] Iteration[1800/2001] Loss: 0.0571 Acc:98.02%
Epoch[154/200] Train Acc: 98.03% Valid Acc:97.38% Train loss:0.0569 Valid loss:0.0823 Train fpr:0.01% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[155/200] Iteration[300/2001] Loss: 0.0613 Acc:97.99%
Training: Epoch[155/200] Iteration[600/2001] Loss: 0.0592 Acc:98.00%
Training: Epoch[155/200] Iteration[900/2001] Loss: 0.0584 Acc:98.05%
Training: Epoch[155/200] Iteration[1200/2001] Loss: 0.0576 Acc:98.11%
Training: Epoch[155/200] Iteration[1500/2001] Loss: 0.0567 Acc:98.11%
Training: Epoch[155/200] Iteration[1800/2001] Loss: 0.0557 Acc:98.14%
Epoch[155/200] Train Acc: 98.14% Valid Acc:97.41% Train loss:0.0554 Valid loss:0.0840 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[156/200] Iteration[300/2001] Loss: 0.0550 Acc:98.06%
Training: Epoch[156/200] Iteration[600/2001] Loss: 0.0525 Acc:98.16%
Training: Epoch[156/200] Iteration[900/2001] Loss: 0.0519 Acc:98.18%
Training: Epoch[156/200] Iteration[1200/2001] Loss: 0.0530 Acc:98.14%
Training: Epoch[156/200] Iteration[1500/2001] Loss: 0.0539 Acc:98.14%
Training: Epoch[156/200] Iteration[1800/2001] Loss: 0.0545 Acc:98.12%
Epoch[156/200] Train Acc: 98.11% Valid Acc:97.34% Train loss:0.0547 Valid loss:0.0822 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
=>Best2 model updated
Training: Epoch[157/200] Iteration[300/2001] Loss: 0.0532 Acc:98.31%
Training: Epoch[157/200] Iteration[600/2001] Loss: 0.0557 Acc:98.12%
Training: Epoch[157/200] Iteration[900/2001] Loss: 0.0547 Acc:98.16%
Training: Epoch[157/200] Iteration[1200/2001] Loss: 0.0552 Acc:98.14%
Training: Epoch[157/200] Iteration[1500/2001] Loss: 0.0551 Acc:98.15%
Training: Epoch[157/200] Iteration[1800/2001] Loss: 0.0553 Acc:98.16%
Epoch[157/200] Train Acc: 98.17% Valid Acc:97.36% Train loss:0.0553 Valid loss:0.0840 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
=>Best1 model updated
Training: Epoch[158/200] Iteration[300/2001] Loss: 0.0530 Acc:98.18%
Training: Epoch[158/200] Iteration[600/2001] Loss: 0.0526 Acc:98.21%
Training: Epoch[158/200] Iteration[900/2001] Loss: 0.0502 Acc:98.28%
Training: Epoch[158/200] Iteration[1200/2001] Loss: 0.0503 Acc:98.29%
Training: Epoch[158/200] Iteration[1500/2001] Loss: 0.0515 Acc:98.22%
Training: Epoch[158/200] Iteration[1800/2001] Loss: 0.0519 Acc:98.21%
Epoch[158/200] Train Acc: 98.20% Valid Acc:97.28% Train loss:0.0529 Valid loss:0.0867 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
=>Best1 model updated
Training: Epoch[159/200] Iteration[300/2001] Loss: 0.0524 Acc:98.19%
Training: Epoch[159/200] Iteration[600/2001] Loss: 0.0550 Acc:98.18%
Training: Epoch[159/200] Iteration[900/2001] Loss: 0.0519 Acc:98.27%
Training: Epoch[159/200] Iteration[1200/2001] Loss: 0.0530 Acc:98.21%
Training: Epoch[159/200] Iteration[1500/2001] Loss: 0.0514 Acc:98.27%
Training: Epoch[159/200] Iteration[1800/2001] Loss: 0.0513 Acc:98.27%
Epoch[159/200] Train Acc: 98.24% Valid Acc:97.34% Train loss:0.0522 Valid loss:0.0841 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
=>Best2 model updated
Training: Epoch[160/200] Iteration[300/2001] Loss: 0.0464 Acc:98.26%
Training: Epoch[160/200] Iteration[600/2001] Loss: 0.0490 Acc:98.28%
Training: Epoch[160/200] Iteration[900/2001] Loss: 0.0499 Acc:98.27%
Training: Epoch[160/200] Iteration[1200/2001] Loss: 0.0491 Acc:98.31%
Training: Epoch[160/200] Iteration[1500/2001] Loss: 0.0499 Acc:98.26%
Training: Epoch[160/200] Iteration[1800/2001] Loss: 0.0508 Acc:98.26%
Epoch[160/200] Train Acc: 98.24% Valid Acc:97.44% Train loss:0.0512 Valid loss:0.0846 Train fpr:0.01% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[161/200] Iteration[300/2001] Loss: 0.0536 Acc:98.23%
Training: Epoch[161/200] Iteration[600/2001] Loss: 0.0510 Acc:98.31%
Training: Epoch[161/200] Iteration[900/2001] Loss: 0.0491 Acc:98.33%
Training: Epoch[161/200] Iteration[1200/2001] Loss: 0.0491 Acc:98.34%
Training: Epoch[161/200] Iteration[1500/2001] Loss: 0.0496 Acc:98.32%
Training: Epoch[161/200] Iteration[1800/2001] Loss: 0.0496 Acc:98.31%
Epoch[161/200] Train Acc: 98.32% Valid Acc:97.38% Train loss:0.0494 Valid loss:0.0867 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
=>Best3 model updated
Training: Epoch[162/200] Iteration[300/2001] Loss: 0.0469 Acc:98.31%
Training: Epoch[162/200] Iteration[600/2001] Loss: 0.0516 Acc:98.22%
Training: Epoch[162/200] Iteration[900/2001] Loss: 0.0518 Acc:98.22%
Training: Epoch[162/200] Iteration[1200/2001] Loss: 0.0503 Acc:98.27%
Training: Epoch[162/200] Iteration[1500/2001] Loss: 0.0497 Acc:98.29%
Training: Epoch[162/200] Iteration[1800/2001] Loss: 0.0500 Acc:98.27%
Epoch[162/200] Train Acc: 98.27% Valid Acc:97.34% Train loss:0.0503 Valid loss:0.0873 Train fpr:0.01% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
=>Best2 model updated
Training: Epoch[163/200] Iteration[300/2001] Loss: 0.0425 Acc:98.67%
Training: Epoch[163/200] Iteration[600/2001] Loss: 0.0461 Acc:98.47%
Training: Epoch[163/200] Iteration[900/2001] Loss: 0.0469 Acc:98.40%
Training: Epoch[163/200] Iteration[1200/2001] Loss: 0.0486 Acc:98.34%
Training: Epoch[163/200] Iteration[1500/2001] Loss: 0.0480 Acc:98.34%
Training: Epoch[163/200] Iteration[1800/2001] Loss: 0.0490 Acc:98.29%
Epoch[163/200] Train Acc: 98.29% Valid Acc:97.16% Train loss:0.0491 Valid loss:0.0891 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[164/200] Iteration[300/2001] Loss: 0.0544 Acc:98.17%
Training: Epoch[164/200] Iteration[600/2001] Loss: 0.0517 Acc:98.24%
Training: Epoch[164/200] Iteration[900/2001] Loss: 0.0503 Acc:98.31%
Training: Epoch[164/200] Iteration[1200/2001] Loss: 0.0492 Acc:98.34%
Training: Epoch[164/200] Iteration[1500/2001] Loss: 0.0505 Acc:98.31%
Training: Epoch[164/200] Iteration[1800/2001] Loss: 0.0498 Acc:98.33%
Epoch[164/200] Train Acc: 98.34% Valid Acc:97.37% Train loss:0.0492 Valid loss:0.0874 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[165/200] Iteration[300/2001] Loss: 0.0482 Acc:98.38%
Training: Epoch[165/200] Iteration[600/2001] Loss: 0.0462 Acc:98.41%
Training: Epoch[165/200] Iteration[900/2001] Loss: 0.0488 Acc:98.31%
Training: Epoch[165/200] Iteration[1200/2001] Loss: 0.0488 Acc:98.29%
Training: Epoch[165/200] Iteration[1500/2001] Loss: 0.0489 Acc:98.28%
Training: Epoch[165/200] Iteration[1800/2001] Loss: 0.0483 Acc:98.30%
Epoch[165/200] Train Acc: 98.31% Valid Acc:97.26% Train loss:0.0484 Valid loss:0.0897 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[166/200] Iteration[300/2001] Loss: 0.0479 Acc:98.36%
Training: Epoch[166/200] Iteration[600/2001] Loss: 0.0465 Acc:98.39%
Training: Epoch[166/200] Iteration[900/2001] Loss: 0.0463 Acc:98.44%
Training: Epoch[166/200] Iteration[1200/2001] Loss: 0.0469 Acc:98.41%
Training: Epoch[166/200] Iteration[1500/2001] Loss: 0.0467 Acc:98.41%
Training: Epoch[166/200] Iteration[1800/2001] Loss: 0.0475 Acc:98.38%
Epoch[166/200] Train Acc: 98.39% Valid Acc:97.41% Train loss:0.0468 Valid loss:0.0904 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[167/200] Iteration[300/2001] Loss: 0.0465 Acc:98.34%
Training: Epoch[167/200] Iteration[600/2001] Loss: 0.0470 Acc:98.42%
Training: Epoch[167/200] Iteration[900/2001] Loss: 0.0458 Acc:98.43%
Training: Epoch[167/200] Iteration[1200/2001] Loss: 0.0469 Acc:98.41%
Training: Epoch[167/200] Iteration[1500/2001] Loss: 0.0468 Acc:98.40%
Training: Epoch[167/200] Iteration[1800/2001] Loss: 0.0472 Acc:98.39%
Epoch[167/200] Train Acc: 98.39% Valid Acc:97.24% Train loss:0.0472 Valid loss:0.0915 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[168/200] Iteration[300/2001] Loss: 0.0465 Acc:98.29%
Training: Epoch[168/200] Iteration[600/2001] Loss: 0.0459 Acc:98.38%
Training: Epoch[168/200] Iteration[900/2001] Loss: 0.0462 Acc:98.39%
Training: Epoch[168/200] Iteration[1200/2001] Loss: 0.0462 Acc:98.39%
Training: Epoch[168/200] Iteration[1500/2001] Loss: 0.0461 Acc:98.39%
Training: Epoch[168/200] Iteration[1800/2001] Loss: 0.0468 Acc:98.36%
Epoch[168/200] Train Acc: 98.38% Valid Acc:97.38% Train loss:0.0463 Valid loss:0.0919 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[169/200] Iteration[300/2001] Loss: 0.0448 Acc:98.44%
Training: Epoch[169/200] Iteration[600/2001] Loss: 0.0461 Acc:98.39%
Training: Epoch[169/200] Iteration[900/2001] Loss: 0.0462 Acc:98.40%
Training: Epoch[169/200] Iteration[1200/2001] Loss: 0.0461 Acc:98.41%
Training: Epoch[169/200] Iteration[1500/2001] Loss: 0.0455 Acc:98.43%
Training: Epoch[169/200] Iteration[1800/2001] Loss: 0.0452 Acc:98.43%
Epoch[169/200] Train Acc: 98.42% Valid Acc:97.38% Train loss:0.0453 Valid loss:0.0905 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:100.00% LR:0.0010000000000000002
=>Best2 model updated
Training: Epoch[170/200] Iteration[300/2001] Loss: 0.0442 Acc:98.54%
Training: Epoch[170/200] Iteration[600/2001] Loss: 0.0458 Acc:98.44%
Training: Epoch[170/200] Iteration[900/2001] Loss: 0.0461 Acc:98.45%
Training: Epoch[170/200] Iteration[1200/2001] Loss: 0.0466 Acc:98.45%
Training: Epoch[170/200] Iteration[1500/2001] Loss: 0.0462 Acc:98.44%
Training: Epoch[170/200] Iteration[1800/2001] Loss: 0.0466 Acc:98.43%
Epoch[170/200] Train Acc: 98.43% Valid Acc:97.36% Train loss:0.0462 Valid loss:0.0904 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[171/200] Iteration[300/2001] Loss: 0.0413 Acc:98.51%
Training: Epoch[171/200] Iteration[600/2001] Loss: 0.0439 Acc:98.46%
Training: Epoch[171/200] Iteration[900/2001] Loss: 0.0430 Acc:98.55%
Training: Epoch[171/200] Iteration[1200/2001] Loss: 0.0429 Acc:98.53%
Training: Epoch[171/200] Iteration[1500/2001] Loss: 0.0433 Acc:98.52%
Training: Epoch[171/200] Iteration[1800/2001] Loss: 0.0439 Acc:98.48%
Epoch[171/200] Train Acc: 98.48% Valid Acc:97.38% Train loss:0.0443 Valid loss:0.0904 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[172/200] Iteration[300/2001] Loss: 0.0431 Acc:98.42%
Training: Epoch[172/200] Iteration[600/2001] Loss: 0.0412 Acc:98.56%
Training: Epoch[172/200] Iteration[900/2001] Loss: 0.0424 Acc:98.55%
Training: Epoch[172/200] Iteration[1200/2001] Loss: 0.0427 Acc:98.54%
Training: Epoch[172/200] Iteration[1500/2001] Loss: 0.0436 Acc:98.53%
Training: Epoch[172/200] Iteration[1800/2001] Loss: 0.0427 Acc:98.56%
Epoch[172/200] Train Acc: 98.57% Valid Acc:97.23% Train loss:0.0428 Valid loss:0.0968 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[173/200] Iteration[300/2001] Loss: 0.0450 Acc:98.34%
Training: Epoch[173/200] Iteration[600/2001] Loss: 0.0462 Acc:98.44%
Training: Epoch[173/200] Iteration[900/2001] Loss: 0.0452 Acc:98.47%
Training: Epoch[173/200] Iteration[1200/2001] Loss: 0.0447 Acc:98.49%
Training: Epoch[173/200] Iteration[1500/2001] Loss: 0.0443 Acc:98.50%
Training: Epoch[173/200] Iteration[1800/2001] Loss: 0.0451 Acc:98.46%
Epoch[173/200] Train Acc: 98.50% Valid Acc:97.36% Train loss:0.0444 Valid loss:0.0943 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[174/200] Iteration[300/2001] Loss: 0.0461 Acc:98.49%
Training: Epoch[174/200] Iteration[600/2001] Loss: 0.0459 Acc:98.51%
Training: Epoch[174/200] Iteration[900/2001] Loss: 0.0454 Acc:98.50%
Training: Epoch[174/200] Iteration[1200/2001] Loss: 0.0455 Acc:98.50%
Training: Epoch[174/200] Iteration[1500/2001] Loss: 0.0445 Acc:98.51%
Training: Epoch[174/200] Iteration[1800/2001] Loss: 0.0440 Acc:98.51%
Epoch[174/200] Train Acc: 98.52% Valid Acc:97.32% Train loss:0.0438 Valid loss:0.0961 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[175/200] Iteration[300/2001] Loss: 0.0387 Acc:98.62%
Training: Epoch[175/200] Iteration[600/2001] Loss: 0.0382 Acc:98.66%
Training: Epoch[175/200] Iteration[900/2001] Loss: 0.0404 Acc:98.57%
Training: Epoch[175/200] Iteration[1200/2001] Loss: 0.0421 Acc:98.53%
Training: Epoch[175/200] Iteration[1500/2001] Loss: 0.0431 Acc:98.52%
Training: Epoch[175/200] Iteration[1800/2001] Loss: 0.0427 Acc:98.53%
Epoch[175/200] Train Acc: 98.52% Valid Acc:97.26% Train loss:0.0432 Valid loss:0.0933 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[176/200] Iteration[300/2001] Loss: 0.0432 Acc:98.56%
Training: Epoch[176/200] Iteration[600/2001] Loss: 0.0443 Acc:98.54%
Training: Epoch[176/200] Iteration[900/2001] Loss: 0.0441 Acc:98.57%
Training: Epoch[176/200] Iteration[1200/2001] Loss: 0.0425 Acc:98.61%
Training: Epoch[176/200] Iteration[1500/2001] Loss: 0.0425 Acc:98.58%
Training: Epoch[176/200] Iteration[1800/2001] Loss: 0.0433 Acc:98.53%
Epoch[176/200] Train Acc: 98.56% Valid Acc:97.43% Train loss:0.0427 Valid loss:0.0906 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[177/200] Iteration[300/2001] Loss: 0.0439 Acc:98.59%
Training: Epoch[177/200] Iteration[600/2001] Loss: 0.0416 Acc:98.68%
Training: Epoch[177/200] Iteration[900/2001] Loss: 0.0404 Acc:98.66%
Training: Epoch[177/200] Iteration[1200/2001] Loss: 0.0403 Acc:98.64%
Training: Epoch[177/200] Iteration[1500/2001] Loss: 0.0413 Acc:98.59%
Training: Epoch[177/200] Iteration[1800/2001] Loss: 0.0412 Acc:98.59%
Epoch[177/200] Train Acc: 98.59% Valid Acc:97.43% Train loss:0.0416 Valid loss:0.0925 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[178/200] Iteration[300/2001] Loss: 0.0388 Acc:98.62%
Training: Epoch[178/200] Iteration[600/2001] Loss: 0.0410 Acc:98.60%
Training: Epoch[178/200] Iteration[900/2001] Loss: 0.0411 Acc:98.59%
Training: Epoch[178/200] Iteration[1200/2001] Loss: 0.0403 Acc:98.66%
Training: Epoch[178/200] Iteration[1500/2001] Loss: 0.0413 Acc:98.62%
Training: Epoch[178/200] Iteration[1800/2001] Loss: 0.0416 Acc:98.61%
Epoch[178/200] Train Acc: 98.61% Valid Acc:97.24% Train loss:0.0415 Valid loss:0.0969 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[179/200] Iteration[300/2001] Loss: 0.0423 Acc:98.51%
Training: Epoch[179/200] Iteration[600/2001] Loss: 0.0432 Acc:98.47%
Training: Epoch[179/200] Iteration[900/2001] Loss: 0.0433 Acc:98.46%
Training: Epoch[179/200] Iteration[1200/2001] Loss: 0.0430 Acc:98.48%
Training: Epoch[179/200] Iteration[1500/2001] Loss: 0.0415 Acc:98.54%
Training: Epoch[179/200] Iteration[1800/2001] Loss: 0.0412 Acc:98.55%
Epoch[179/200] Train Acc: 98.55% Valid Acc:97.16% Train loss:0.0411 Valid loss:0.0989 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[180/200] Iteration[300/2001] Loss: 0.0377 Acc:98.77%
Training: Epoch[180/200] Iteration[600/2001] Loss: 0.0387 Acc:98.67%
Training: Epoch[180/200] Iteration[900/2001] Loss: 0.0403 Acc:98.62%
Training: Epoch[180/200] Iteration[1200/2001] Loss: 0.0401 Acc:98.64%
Training: Epoch[180/200] Iteration[1500/2001] Loss: 0.0398 Acc:98.65%
Training: Epoch[180/200] Iteration[1800/2001] Loss: 0.0406 Acc:98.60%
Epoch[180/200] Train Acc: 98.58% Valid Acc:97.24% Train loss:0.0411 Valid loss:0.0991 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[181/200] Iteration[300/2001] Loss: 0.0481 Acc:98.45%
Training: Epoch[181/200] Iteration[600/2001] Loss: 0.0436 Acc:98.61%
Training: Epoch[181/200] Iteration[900/2001] Loss: 0.0426 Acc:98.55%
Training: Epoch[181/200] Iteration[1200/2001] Loss: 0.0421 Acc:98.55%
Training: Epoch[181/200] Iteration[1500/2001] Loss: 0.0427 Acc:98.51%
Training: Epoch[181/200] Iteration[1800/2001] Loss: 0.0422 Acc:98.52%
Epoch[181/200] Train Acc: 98.55% Valid Acc:97.34% Train loss:0.0418 Valid loss:0.0989 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[182/200] Iteration[300/2001] Loss: 0.0372 Acc:98.75%
Training: Epoch[182/200] Iteration[600/2001] Loss: 0.0383 Acc:98.66%
Training: Epoch[182/200] Iteration[900/2001] Loss: 0.0400 Acc:98.64%
Training: Epoch[182/200] Iteration[1200/2001] Loss: 0.0418 Acc:98.55%
Training: Epoch[182/200] Iteration[1500/2001] Loss: 0.0409 Acc:98.57%
Training: Epoch[182/200] Iteration[1800/2001] Loss: 0.0405 Acc:98.58%
Epoch[182/200] Train Acc: 98.60% Valid Acc:97.14% Train loss:0.0402 Valid loss:0.1037 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[183/200] Iteration[300/2001] Loss: 0.0402 Acc:98.70%
Training: Epoch[183/200] Iteration[600/2001] Loss: 0.0393 Acc:98.64%
Training: Epoch[183/200] Iteration[900/2001] Loss: 0.0391 Acc:98.65%
Training: Epoch[183/200] Iteration[1200/2001] Loss: 0.0389 Acc:98.65%
Training: Epoch[183/200] Iteration[1500/2001] Loss: 0.0399 Acc:98.62%
Training: Epoch[183/200] Iteration[1800/2001] Loss: 0.0388 Acc:98.66%
Epoch[183/200] Train Acc: 98.67% Valid Acc:97.22% Train loss:0.0388 Valid loss:0.1014 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[184/200] Iteration[300/2001] Loss: 0.0402 Acc:98.73%
Training: Epoch[184/200] Iteration[600/2001] Loss: 0.0399 Acc:98.75%
Training: Epoch[184/200] Iteration[900/2001] Loss: 0.0386 Acc:98.77%
Training: Epoch[184/200] Iteration[1200/2001] Loss: 0.0383 Acc:98.73%
Training: Epoch[184/200] Iteration[1500/2001] Loss: 0.0388 Acc:98.71%
Training: Epoch[184/200] Iteration[1800/2001] Loss: 0.0382 Acc:98.72%
Epoch[184/200] Train Acc: 98.69% Valid Acc:97.32% Train loss:0.0390 Valid loss:0.1010 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[185/200] Iteration[300/2001] Loss: 0.0413 Acc:98.67%
Training: Epoch[185/200] Iteration[600/2001] Loss: 0.0354 Acc:98.88%
Training: Epoch[185/200] Iteration[900/2001] Loss: 0.0364 Acc:98.78%
Training: Epoch[185/200] Iteration[1200/2001] Loss: 0.0373 Acc:98.75%
Training: Epoch[185/200] Iteration[1500/2001] Loss: 0.0384 Acc:98.69%
Training: Epoch[185/200] Iteration[1800/2001] Loss: 0.0391 Acc:98.68%
Epoch[185/200] Train Acc: 98.67% Valid Acc:97.36% Train loss:0.0392 Valid loss:0.1004 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[186/200] Iteration[300/2001] Loss: 0.0369 Acc:98.78%
Training: Epoch[186/200] Iteration[600/2001] Loss: 0.0380 Acc:98.70%
Training: Epoch[186/200] Iteration[900/2001] Loss: 0.0374 Acc:98.73%
Training: Epoch[186/200] Iteration[1200/2001] Loss: 0.0382 Acc:98.67%
Training: Epoch[186/200] Iteration[1500/2001] Loss: 0.0383 Acc:98.66%
Training: Epoch[186/200] Iteration[1800/2001] Loss: 0.0384 Acc:98.66%
Epoch[186/200] Train Acc: 98.65% Valid Acc:97.26% Train loss:0.0386 Valid loss:0.1024 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[187/200] Iteration[300/2001] Loss: 0.0356 Acc:98.68%
Training: Epoch[187/200] Iteration[600/2001] Loss: 0.0373 Acc:98.74%
Training: Epoch[187/200] Iteration[900/2001] Loss: 0.0368 Acc:98.77%
Training: Epoch[187/200] Iteration[1200/2001] Loss: 0.0356 Acc:98.81%
Training: Epoch[187/200] Iteration[1500/2001] Loss: 0.0373 Acc:98.76%
Training: Epoch[187/200] Iteration[1800/2001] Loss: 0.0376 Acc:98.75%
Epoch[187/200] Train Acc: 98.72% Valid Acc:97.22% Train loss:0.0378 Valid loss:0.0982 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:100.00% LR:0.0010000000000000002
=>Best1 model updated
Training: Epoch[188/200] Iteration[300/2001] Loss: 0.0326 Acc:98.80%
Training: Epoch[188/200] Iteration[600/2001] Loss: 0.0348 Acc:98.79%
Training: Epoch[188/200] Iteration[900/2001] Loss: 0.0352 Acc:98.78%
Training: Epoch[188/200] Iteration[1200/2001] Loss: 0.0356 Acc:98.77%
Training: Epoch[188/200] Iteration[1500/2001] Loss: 0.0363 Acc:98.72%
Training: Epoch[188/200] Iteration[1800/2001] Loss: 0.0364 Acc:98.73%
Epoch[188/200] Train Acc: 98.71% Valid Acc:97.24% Train loss:0.0370 Valid loss:0.1014 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:100.00% LR:0.0010000000000000002
=>Best1 model updated
Training: Epoch[189/200] Iteration[300/2001] Loss: 0.0378 Acc:98.75%
Training: Epoch[189/200] Iteration[600/2001] Loss: 0.0365 Acc:98.78%
Training: Epoch[189/200] Iteration[900/2001] Loss: 0.0355 Acc:98.79%
Training: Epoch[189/200] Iteration[1200/2001] Loss: 0.0361 Acc:98.81%
Training: Epoch[189/200] Iteration[1500/2001] Loss: 0.0365 Acc:98.82%
Training: Epoch[189/200] Iteration[1800/2001] Loss: 0.0369 Acc:98.81%
Epoch[189/200] Train Acc: 98.79% Valid Acc:96.97% Train loss:0.0373 Valid loss:0.1067 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[190/200] Iteration[300/2001] Loss: 0.0311 Acc:98.94%
Training: Epoch[190/200] Iteration[600/2001] Loss: 0.0356 Acc:98.80%
Training: Epoch[190/200] Iteration[900/2001] Loss: 0.0366 Acc:98.74%
Training: Epoch[190/200] Iteration[1200/2001] Loss: 0.0366 Acc:98.73%
Training: Epoch[190/200] Iteration[1500/2001] Loss: 0.0367 Acc:98.74%
Training: Epoch[190/200] Iteration[1800/2001] Loss: 0.0373 Acc:98.73%
Epoch[190/200] Train Acc: 98.73% Valid Acc:97.22% Train loss:0.0372 Valid loss:0.1052 Train fpr:0.01% Valid fpr:0.00% Train AUC:100.00% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[191/200] Iteration[300/2001] Loss: 0.0346 Acc:98.83%
Training: Epoch[191/200] Iteration[600/2001] Loss: 0.0360 Acc:98.81%
Training: Epoch[191/200] Iteration[900/2001] Loss: 0.0357 Acc:98.81%
Training: Epoch[191/200] Iteration[1200/2001] Loss: 0.0366 Acc:98.79%
Training: Epoch[191/200] Iteration[1500/2001] Loss: 0.0375 Acc:98.76%
Training: Epoch[191/200] Iteration[1800/2001] Loss: 0.0379 Acc:98.76%
Epoch[191/200] Train Acc: 98.74% Valid Acc:96.92% Train loss:0.0384 Valid loss:0.1053 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[192/200] Iteration[300/2001] Loss: 0.0398 Acc:98.70%
Training: Epoch[192/200] Iteration[600/2001] Loss: 0.0396 Acc:98.65%
Training: Epoch[192/200] Iteration[900/2001] Loss: 0.0364 Acc:98.78%
Training: Epoch[192/200] Iteration[1200/2001] Loss: 0.0359 Acc:98.78%
Training: Epoch[192/200] Iteration[1500/2001] Loss: 0.0367 Acc:98.75%
Training: Epoch[192/200] Iteration[1800/2001] Loss: 0.0363 Acc:98.76%
Epoch[192/200] Train Acc: 98.76% Valid Acc:97.11% Train loss:0.0369 Valid loss:0.1050 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[193/200] Iteration[300/2001] Loss: 0.0363 Acc:98.77%
Training: Epoch[193/200] Iteration[600/2001] Loss: 0.0353 Acc:98.83%
Training: Epoch[193/200] Iteration[900/2001] Loss: 0.0343 Acc:98.82%
Training: Epoch[193/200] Iteration[1200/2001] Loss: 0.0351 Acc:98.77%
Training: Epoch[193/200] Iteration[1500/2001] Loss: 0.0354 Acc:98.75%
Training: Epoch[193/200] Iteration[1800/2001] Loss: 0.0351 Acc:98.78%
Epoch[193/200] Train Acc: 98.78% Valid Acc:97.08% Train loss:0.0356 Valid loss:0.1088 Train fpr:0.01% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[194/200] Iteration[300/2001] Loss: 0.0335 Acc:98.90%
Training: Epoch[194/200] Iteration[600/2001] Loss: 0.0372 Acc:98.71%
Training: Epoch[194/200] Iteration[900/2001] Loss: 0.0365 Acc:98.74%
Training: Epoch[194/200] Iteration[1200/2001] Loss: 0.0363 Acc:98.76%
Training: Epoch[194/200] Iteration[1500/2001] Loss: 0.0366 Acc:98.74%
Training: Epoch[194/200] Iteration[1800/2001] Loss: 0.0364 Acc:98.73%
Epoch[194/200] Train Acc: 98.72% Valid Acc:97.14% Train loss:0.0367 Valid loss:0.1082 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[195/200] Iteration[300/2001] Loss: 0.0369 Acc:98.67%
Training: Epoch[195/200] Iteration[600/2001] Loss: 0.0368 Acc:98.76%
Training: Epoch[195/200] Iteration[900/2001] Loss: 0.0362 Acc:98.76%
Training: Epoch[195/200] Iteration[1200/2001] Loss: 0.0361 Acc:98.80%
Training: Epoch[195/200] Iteration[1500/2001] Loss: 0.0364 Acc:98.80%
Training: Epoch[195/200] Iteration[1800/2001] Loss: 0.0364 Acc:98.80%
Epoch[195/200] Train Acc: 98.79% Valid Acc:97.06% Train loss:0.0365 Valid loss:0.1047 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:100.00% LR:0.0010000000000000002
Training: Epoch[196/200] Iteration[300/2001] Loss: 0.0351 Acc:98.79%
Training: Epoch[196/200] Iteration[600/2001] Loss: 0.0339 Acc:98.83%
Training: Epoch[196/200] Iteration[900/2001] Loss: 0.0356 Acc:98.77%
Training: Epoch[196/200] Iteration[1200/2001] Loss: 0.0361 Acc:98.76%
Training: Epoch[196/200] Iteration[1500/2001] Loss: 0.0358 Acc:98.78%
Training: Epoch[196/200] Iteration[1800/2001] Loss: 0.0349 Acc:98.81%
Epoch[196/200] Train Acc: 98.81% Valid Acc:97.17% Train loss:0.0348 Valid loss:0.1055 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:100.00% LR:0.0010000000000000002
=>Best3 model updated
Training: Epoch[197/200] Iteration[300/2001] Loss: 0.0280 Acc:98.97%
Training: Epoch[197/200] Iteration[600/2001] Loss: 0.0318 Acc:98.89%
Training: Epoch[197/200] Iteration[900/2001] Loss: 0.0320 Acc:98.88%
Training: Epoch[197/200] Iteration[1200/2001] Loss: 0.0327 Acc:98.86%
Training: Epoch[197/200] Iteration[1500/2001] Loss: 0.0322 Acc:98.88%
Training: Epoch[197/200] Iteration[1800/2001] Loss: 0.0321 Acc:98.88%
Epoch[197/200] Train Acc: 98.86% Valid Acc:97.16% Train loss:0.0329 Valid loss:0.1071 Train fpr:0.01% Valid fpr:0.00% Train AUC:100.00% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[198/200] Iteration[300/2001] Loss: 0.0347 Acc:98.78%
Training: Epoch[198/200] Iteration[600/2001] Loss: 0.0356 Acc:98.78%
Training: Epoch[198/200] Iteration[900/2001] Loss: 0.0361 Acc:98.77%
Training: Epoch[198/200] Iteration[1200/2001] Loss: 0.0352 Acc:98.82%
Training: Epoch[198/200] Iteration[1500/2001] Loss: 0.0349 Acc:98.83%
Training: Epoch[198/200] Iteration[1800/2001] Loss: 0.0349 Acc:98.84%
Epoch[198/200] Train Acc: 98.81% Valid Acc:97.13% Train loss:0.0354 Valid loss:0.1095 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[199/200] Iteration[300/2001] Loss: 0.0355 Acc:98.85%
Training: Epoch[199/200] Iteration[600/2001] Loss: 0.0336 Acc:98.83%
Training: Epoch[199/200] Iteration[900/2001] Loss: 0.0332 Acc:98.84%
Training: Epoch[199/200] Iteration[1200/2001] Loss: 0.0342 Acc:98.80%
Training: Epoch[199/200] Iteration[1500/2001] Loss: 0.0349 Acc:98.79%
Training: Epoch[199/200] Iteration[1800/2001] Loss: 0.0345 Acc:98.82%
Epoch[199/200] Train Acc: 98.82% Valid Acc:97.21% Train loss:0.0344 Valid loss:0.1068 Train fpr:0.00% Valid fpr:0.00% Train AUC:99.99% Valid AUC:99.99% LR:0.0010000000000000002
Training: Epoch[200/200] Iteration[300/2001] Loss: 0.0327 Acc:98.91%
Training: Epoch[200/200] Iteration[600/2001] Loss: 0.0326 Acc:98.92%
Training: Epoch[200/200] Iteration[900/2001] Loss: 0.0327 Acc:98.92%
Training: Epoch[200/200] Iteration[1200/2001] Loss: 0.0333 Acc:98.87%
Training: Epoch[200/200] Iteration[1500/2001] Loss: 0.0344 Acc:98.83%
Training: Epoch[200/200] Iteration[1800/2001] Loss: 0.0347 Acc:98.81%
Epoch[200/200] Train Acc: 98.81% Valid Acc:97.29% Train loss:0.0349 Valid loss:0.1031 Train fpr:0.00% Valid fpr:0.00% Train AUC:100.00% Valid AUC:99.99% LR:0.0010000000000000002
 done ~~~~ 05-04_02-45, best auc: 0.9999852731256635 in :187 epochs. 
05-04_02-45
